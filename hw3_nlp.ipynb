{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iHb2ls24vOdZ"
      ],
      "authorship_tag": "ABX9TyMErXj0KvyKcXWEFW9M9IFh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varchala/NaturalLanguageProcessing_CSC8980/blob/main/hw3_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlIsmK30ilhf"
      },
      "source": [
        "**G Varchaleswari**\r\n",
        "\r\n",
        "vganugapati1@student.gsu.edu\r\n",
        "\r\n",
        "**002575320**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx-uZRvohYYq"
      },
      "source": [
        "# 1. \r\n",
        "Using NLTK tokenize all documents, separated by polarity, remove stop words, and list\r\n",
        "the top 20 most frequent tokens (and their counts) for the positive reviews, and the top\r\n",
        "20 most frequent tokens (and their counts). What kind of things do you notice are\r\n",
        "different between the two sets? (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljPzB10YjtpB"
      },
      "source": [
        "# !tar -xvf /content/sample_data/review_polarity.tar -C /content/sample_data/"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYK2e1n-g6ft"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import os"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csevxDp0kgeq"
      },
      "source": [
        "def file_extract(data,dir):\r\n",
        "  path = \"/content/sample_data/txt_sentoken/\"\r\n",
        "  path += dir\r\n",
        "  with os.scandir(path) as files:\r\n",
        "    for file in files:\r\n",
        "      if file.name.endswith(\".txt\"):\r\n",
        "        with open(path+\"/\"+file.name) as review:\r\n",
        "          data = data.append({'data':review.read(),'labels':dir,'name':file.name},ignore_index=True)\r\n",
        "  return data"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh3oz5T1m4AG"
      },
      "source": [
        "df = pd.DataFrame()\r\n",
        "data_df=file_extract(df,\"neg\")\r\n",
        "data_df = file_extract(data_df,\"pos\")"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "gPMKyztEnNZO",
        "outputId": "236384e8-a4b4-4a12-fbe2-b644a1aaf2dd"
      },
      "source": [
        "data_df"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>an attempt at florida film noir , palmetto fai...</td>\n",
              "      <td>neg</td>\n",
              "      <td>cv496_11185.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a hotshot lawyer gets an obviously guilty chil...</td>\n",
              "      <td>neg</td>\n",
              "      <td>cv265_11625.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chill factor is a carbon copy of speed with on...</td>\n",
              "      <td>neg</td>\n",
              "      <td>cv436_20564.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mugshot ( director/writer/cinematographer/edit...</td>\n",
              "      <td>neg</td>\n",
              "      <td>cv708_28539.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i never understood what the clich ? \" hell on ...</td>\n",
              "      <td>neg</td>\n",
              "      <td>cv130_18521.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>one of the sweetest tales to ever be made , it...</td>\n",
              "      <td>pos</td>\n",
              "      <td>cv521_15828.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>after the terminally bleak reservoir dogs and ...</td>\n",
              "      <td>pos</td>\n",
              "      <td>cv822_20049.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>`run lola run' , a german import that gained a...</td>\n",
              "      <td>pos</td>\n",
              "      <td>cv710_22577.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>in october of 1962 the united states found its...</td>\n",
              "      <td>pos</td>\n",
              "      <td>cv771_28665.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>note : some may consider portions of the follo...</td>\n",
              "      <td>pos</td>\n",
              "      <td>cv162_10424.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   data labels             name\n",
              "0     an attempt at florida film noir , palmetto fai...    neg  cv496_11185.txt\n",
              "1     a hotshot lawyer gets an obviously guilty chil...    neg  cv265_11625.txt\n",
              "2     chill factor is a carbon copy of speed with on...    neg  cv436_20564.txt\n",
              "3     mugshot ( director/writer/cinematographer/edit...    neg  cv708_28539.txt\n",
              "4     i never understood what the clich ? \" hell on ...    neg  cv130_18521.txt\n",
              "...                                                 ...    ...              ...\n",
              "1995  one of the sweetest tales to ever be made , it...    pos  cv521_15828.txt\n",
              "1996  after the terminally bleak reservoir dogs and ...    pos  cv822_20049.txt\n",
              "1997  `run lola run' , a german import that gained a...    pos  cv710_22577.txt\n",
              "1998  in october of 1962 the united states found its...    pos  cv771_28665.txt\n",
              "1999  note : some may consider portions of the follo...    pos  cv162_10424.txt\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "YgFwMaRsnvuO",
        "outputId": "0949ea22-ed11-4694-e27d-ac13780e7580"
      },
      "source": [
        "data_df['labels'] = data_df['labels'].map({'pos': 1, 'neg': 0}).astype(int)\r\n",
        "data_df"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>an attempt at florida film noir , palmetto fai...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv496_11185.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a hotshot lawyer gets an obviously guilty chil...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv265_11625.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chill factor is a carbon copy of speed with on...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv436_20564.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mugshot ( director/writer/cinematographer/edit...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv708_28539.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i never understood what the clich ? \" hell on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv130_18521.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>one of the sweetest tales to ever be made , it...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv521_15828.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>after the terminally bleak reservoir dogs and ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv822_20049.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>`run lola run' , a german import that gained a...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv710_22577.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>in october of 1962 the united states found its...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv771_28665.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>note : some may consider portions of the follo...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv162_10424.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   data  ...             name\n",
              "0     an attempt at florida film noir , palmetto fai...  ...  cv496_11185.txt\n",
              "1     a hotshot lawyer gets an obviously guilty chil...  ...  cv265_11625.txt\n",
              "2     chill factor is a carbon copy of speed with on...  ...  cv436_20564.txt\n",
              "3     mugshot ( director/writer/cinematographer/edit...  ...  cv708_28539.txt\n",
              "4     i never understood what the clich ? \" hell on ...  ...  cv130_18521.txt\n",
              "...                                                 ...  ...              ...\n",
              "1995  one of the sweetest tales to ever be made , it...  ...  cv521_15828.txt\n",
              "1996  after the terminally bleak reservoir dogs and ...  ...  cv822_20049.txt\n",
              "1997  `run lola run' , a german import that gained a...  ...  cv710_22577.txt\n",
              "1998  in october of 1962 the united states found its...  ...  cv771_28665.txt\n",
              "1999  note : some may consider portions of the follo...  ...  cv162_10424.txt\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaDSN1E-slPm",
        "outputId": "13fc2f04-20a9-4b51-8eac-23024d2bdbda"
      },
      "source": [
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "s_words = stopwords.words('english')\r\n",
        "print(s_words)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1zh7L7Ltdx7"
      },
      "source": [
        "# Code for removing stop words and counting the the top 20 most frequent words from positive and negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-3GAH6lxMOg",
        "outputId": "3219c0fa-9b3d-4ec7-eb48-bae095b4a7ee"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIUkMdxGV5_G",
        "outputId": "3a9fe54f-26c3-4d95-ce07-6c02738d7bca"
      },
      "source": [
        "def f_words(data,n):\r\n",
        "  words = pd.DataFrame()\r\n",
        "  for sent in data:\r\n",
        "    for word in nltk.word_tokenize(sent):\r\n",
        "      if word not in s_words:\r\n",
        "        if not words.empty and word in words.word.values:\r\n",
        "          words.loc[words.word == word, \"count\"] = words[words['word']==word]['count']+1\r\n",
        "        else:\r\n",
        "          words = words.append({'word':word,'count':1},ignore_index=True)\r\n",
        "  return words.nlargest(n, 'count',keep='all').reset_index(drop=True)\r\n",
        "print(\"Top 20 most frequent words from positive reviews \\n\")\r\n",
        "print(f_words(data_df[data_df['labels']==1]['data'],20))\r\n",
        "print(\"Top 20 most frequent words from negative reviews \\n\")\r\n",
        "print(f_words(data_df[data_df['labels']==0]['data'],20))"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 20 most frequent words from positive reviews \n",
            "\n",
            "      count       word\n",
            "0   42448.0          ,\n",
            "1   33714.0          .\n",
            "2    9473.0         's\n",
            "3    8494.0         ``\n",
            "4    6039.0          )\n",
            "5    6014.0          (\n",
            "6    5186.0       film\n",
            "7    2943.0        one\n",
            "8    2775.0        n't\n",
            "9    2497.0      movie\n",
            "10   1713.0       like\n",
            "11   1570.0          ?\n",
            "12   1502.0          :\n",
            "13   1231.0      story\n",
            "14   1200.0       also\n",
            "15   1190.0       good\n",
            "16   1175.0       even\n",
            "17   1171.0       time\n",
            "18   1079.0      would\n",
            "19   1067.0  character\n",
            "Top 20 most frequent words from negative reviews \n",
            "\n",
            "      count   word\n",
            "0   35269.0      ,\n",
            "1   32162.0      .\n",
            "2    9123.0     ``\n",
            "3    8655.0     's\n",
            "4    5742.0      )\n",
            "5    5650.0      (\n",
            "6    4257.0   film\n",
            "7    3442.0    n't\n",
            "8    3174.0  movie\n",
            "9    2637.0    one\n",
            "10   2201.0      ?\n",
            "11   1832.0   like\n",
            "12   1540.0      :\n",
            "13   1381.0   even\n",
            "14   1185.0  would\n",
            "15   1126.0   good\n",
            "16   1111.0   time\n",
            "17   1056.0      !\n",
            "18   1039.0    get\n",
            "19   1019.0    bad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSv2so4e6K9I"
      },
      "source": [
        "As few tokens like ',.? etc., are also considered in the document, the highest frequencies shown in the both sets are common for these. Also, even after removal of stopwords from NLTK, it still contains some stopwords that are relevant to the context in question i.e. \"Movie reviews\". It will contain some words like film, movie etc which are common in both. Word like \"good\" appeared 1190 times in positive reviews and 1126 times in negative reviews, this could be because good must have appeared as a combination of \"not good\" in negative reviews. Also, if we look at \"n't\" which appeared more in negative reviews than positive, most probable reason could be to express negative sentiments like \"didn't like\" or something like that.Bad appeared 1019 times in negative reviews and is not included in top 20 tokens of positive reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA0pZdC6Cj-x"
      },
      "source": [
        "# 2. \r\n",
        "Using the code from previous lectures, build 3 polarity classifiers using the following\r\n",
        "parameters (20 points). Note: just train the models.\r\n",
        "a) For training: use 50% of the positive dataset and 70% of the negative dataset. For\r\n",
        "your model use: NaiveBayes with the TF-IDF vectorizer.\r\n",
        "b) For training: use 50% of the negative dataset and 70% of the positive dataset. For\r\n",
        "your model use: NaiveBayes with the TF-IDF vectorizer.\r\n",
        "c) For training: use 25% of the negative dataset and 25% of the positive dataset. For\r\n",
        "your model use: SVM with the TF-IDF vectorizer.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuHxtee7s06J"
      },
      "source": [
        "seed_value = 122"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efiYRKF1Dcgg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWAeHqRYsZ_u"
      },
      "source": [
        "def split_set(pos_per,neg_per):\r\n",
        "  a_train,a_test = train_test_split(data_df[data_df['labels']==0], test_size=neg_per, random_state=seed_value)\r\n",
        "  a_train1,a_test1 = train_test_split(data_df[data_df['labels']==1], test_size=pos_per, random_state=seed_value)\r\n",
        "  a_train = a_train.append(a_train1)\r\n",
        "  a_test = a_test.append(a_test1)\r\n",
        "  return a_train,a_test"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI-X3j48C-eq"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\r\n",
        "def train_model(aName,vecName, trainSet):\r\n",
        "  model = make_pipeline(vecName, aName)\r\n",
        "  model.fit(trainSet.data, trainSet.labels)\r\n",
        "  return model"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHb2ls24vOdZ"
      },
      "source": [
        "# 50% of the positive dataset and 70% of the negative dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TnN2oRgDswj"
      },
      "source": [
        "train_a,test_a = split_set(0.50,0.30)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo4zkufTGuOA"
      },
      "source": [
        "# train_a[train_a['labels']==0]"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZlMUCiJvWPO"
      },
      "source": [
        "# NaiveBayes with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEMYpJnvCvJj"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYjCNfOPsR7n"
      },
      "source": [
        "model_a = train_model(MultinomialNB(),TfidfVectorizer(),train_a)"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCCs7g5CIcAL"
      },
      "source": [
        "# 50% of the negative dataset and 70% of the positive dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsmniCUfIXug"
      },
      "source": [
        "train_b,test_b = split_set(0.30,0.50)"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtSl3oB0JEQe"
      },
      "source": [
        "# NaiveBayes with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62uCCh-1JA6r"
      },
      "source": [
        "model_b = train_model(MultinomialNB(),TfidfVectorizer(),train_b)"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z731N7GrJT60"
      },
      "source": [
        "# 25% of the negative dataset and 25% of the positive dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crKd_vY1JZUL"
      },
      "source": [
        "train_c,test_c = split_set(0.75,0.75)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFTbMiMxJxeZ"
      },
      "source": [
        "# SVM with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBx2hFyCJ8aM"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_QN582SJ1y1"
      },
      "source": [
        "model_c = train_model(SVC(),TfidfVectorizer(),train_c)"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fPYxd30Ku5_"
      },
      "source": [
        "# 3.\r\n",
        "Using the models from question 2, evaluate them on their individual rest of the dataset.\r\n",
        "This is, for a) 50% positive and 30% negative, for b) 50% negative and 30% positive, and\r\n",
        "for c) 75% negative and 75% positive. Calculate and show ONLY the following metrics for\r\n",
        "each model: Accuracy, Precision, Recall, Macro F1-score. (15 points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omm1QHywLCUF"
      },
      "source": [
        "import sklearn.metrics as sm\r\n",
        "def evaluate_model(test,model):\r\n",
        "  pred = model.predict(test.data)\r\n",
        "  acc = sm.accuracy_score(test.labels,pred)\r\n",
        "  pr = sm.precision_score(test.labels,pred)\r\n",
        "  re = sm.recall_score(test.labels,pred)\r\n",
        "  f1 = sm.f1_score(pred, test.labels, average='macro')\r\n",
        "  return acc,pr,re,f1"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNAuPQQAMwuH"
      },
      "source": [
        "# a) 50% positive and 30% negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auTEA9GQM1y0"
      },
      "source": [
        "accuracy,precision,recall,f1score=evaluate_model(test_a,model_a)"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srp0B4k7Etux"
      },
      "source": [
        ""
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhBeYy62QiHY",
        "outputId": "5ddbd007-a193-40cf-b55a-aa9b75e29464"
      },
      "source": [
        "print(\"Model a \\n\")\r\n",
        "print('Accuracy:', accuracy)\r\n",
        "print('Precision:', precision)\r\n",
        "print('Recall:', recall)\r\n",
        "print('F1 Score:', f1score)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model a \n",
            "\n",
            "Accuracy: 0.3775\n",
            "Precision: 1.0\n",
            "Recall: 0.004\n",
            "F1 Score: 0.2772081074608669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LssMlL3zQ-bc"
      },
      "source": [
        "# b) 50% negative and 30% positive, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIxChPZ5Q9HD"
      },
      "source": [
        "accuracy,precision,recall,f1score=evaluate_model(test_b,model_b)"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAut0TxQRL3O",
        "outputId": "13997f8c-9ca5-4498-9ffb-be23cd63d708"
      },
      "source": [
        "print(\"Model b \\n\")\r\n",
        "print('Accuracy:', accuracy)\r\n",
        "print('Precision:', precision)\r\n",
        "print('Recall:', recall)\r\n",
        "print('F1 Score:', f1score)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model b \n",
            "\n",
            "Accuracy: 0.375\n",
            "Precision: 0.375\n",
            "Recall: 1.0\n",
            "F1 Score: 0.2727272727272727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69Pf6PpZRZ7t"
      },
      "source": [
        "# c) 75% negative and 75% positive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS7E7WtCRfdQ"
      },
      "source": [
        "accuracy,precision,recall,f1score=evaluate_model(test_c,model_c)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0RVWk71RkFl",
        "outputId": "fc79aee0-84fa-486d-c9ed-b39adba40ece"
      },
      "source": [
        "print(\"Model c \\n\")\r\n",
        "print('Accuracy:', accuracy)\r\n",
        "print('Precision:', precision)\r\n",
        "print('Recall:', recall)\r\n",
        "print('F1 Score:', f1score)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model c \n",
            "\n",
            "Accuracy: 0.7553333333333333\n",
            "Precision: 0.7338217338217338\n",
            "Recall: 0.8013333333333333\n",
            "F1 Score: 0.754814520859472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCxzHirYRzYi"
      },
      "source": [
        "# 4) \r\n",
        "Using the model performance metrics from question 3, answer the following\r\n",
        "questions. Please provide logical and intuitive rationale for your answers, simple\r\n",
        "answers like: because it has the best score, will not be sufficient. (40 points):\r\n",
        "a) What is the best performing model?\r\n",
        "b) Why do you think this is the best performing model?\r\n",
        "c) How does class imbalance play in determining polarity?\r\n",
        "d) Do you think either more data or a better model is a better approach for this\r\n",
        "kind of task?\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G02Vm7X0tpdE"
      },
      "source": [
        "**solution**\r\n",
        "\r\n",
        "Undoubtedly, SVM is the best performing model with metric scores unparalleled to the ones of naive bayes.\r\n",
        "\r\n",
        "But why is it so? If we consider how the algoritm works, Naive bayes works by calculating individual probabilities of features where one observation has no impact on the classification of another observation i.e. the fundamental assumption is that each feature makes an independent and equal contribution to the outcome. SVM is a geometrical non-probabilistic model which works relying on the interdependence of observations in the entire dataset. A hyperplane is drawn such that it separates the dataset into two classes and the orientation of the hyperplane is altered/ adjusted based on the support vector points or the observations that lie closer to the hyperplane. So what this tells us is that, polarity binary classification or sentiment analysis won't work best with probabilistic models as there is a feature to feature interdependence, till an extent, in doing predictions. If a text reads \"The movie was not good and did not satisfy my interests\", we can not consider ['good','satisfy'] and then predict it as positive. There is a relation between \"good\" and \"not\". So this problem is overcome by SVMs till an extent as opposed to Naive bayes algorithm.\r\n",
        "\r\n",
        "Class imbalance introduces certain amount of bias in the data such that the model learns the majority class much better than the minority which results in improper polarity predictions for the minority classes. For example a model predicts the cancerous cells in a body where it was trained on a dataset with more negative classes than positive. When the model is tested on real-time data then predicting a potentially cancerous cell as negative will be a huge loss. This kind of scenarios should be kept in mind while preprocessing or having a dataset with the required number of classes.\r\n",
        "\r\n",
        "Few believe that it is good to have bigger sample space for training better models. But how big is the \"bigger\"? This depends on the kind of model we are building and the context to which it is applied. To make it more sense, it is not always required to have humungous datasets such that we keep increasing the size. This will result in more processing time and a requirement of high-end resources to maintain the same. Techniques like cross validation give us very good estimates of how our model would perform on unseen data and then  based on which we can decide what should be the size of the data i.e., if a dataset is choosen such that applying cross validation results in poor accuracy scores on unseen data then there's a need to update the size of content of the dataset. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja_wE8FzR5iA"
      },
      "source": [
        "# 5) \r\n",
        "Using NLTK and VADER, calculate the sentiment score for all documents in the\r\n",
        "positive polarity. Calculate the polarity threshold needed (and reasonable) to have the\r\n",
        "majority of the document labels match. Do the same for the negative class. Provide the\r\n",
        "threshold needed, the reason why you think this threshold is reasonable, and the\r\n",
        "accuracy percentage (how many documents are correctly labeled using this threshold).\r\n",
        "(45 points):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSS2dTwmRd1_",
        "outputId": "a1f120d3-f901-48fd-de3f-1b8e99f77abe"
      },
      "source": [
        "nltk.download('vader_lexicon')\r\n",
        "nltk.download('movie_reviews')\r\n",
        "nltk.download('punkt')\r\n",
        "!pip install twython"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: twython in /usr/local/lib/python3.7/dist-packages (3.8.2)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVhbaVVxUOZG"
      },
      "source": [
        "data_df_neg = data_df[data_df['labels']==0]"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz_tt82NUeF0"
      },
      "source": [
        "data_df_pos = data_df[data_df['labels']==1].reset_index(drop=True)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Wtp5Dhd7P6"
      },
      "source": [
        "# data_df_pos"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1RE8AeZUhib"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\r\n",
        "\r\n",
        "sia = SIA()"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulr3n1PDiqA2"
      },
      "source": [
        "# Negative polarity sentiment scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "muHTkGRqU0gu",
        "outputId": "3dc77c71-f0a0-4e1d-cd9d-85cada0da4a8"
      },
      "source": [
        "polarity_df1 = pd.DataFrame()\r\n",
        "for i in data_df_neg.data:\r\n",
        "  polarity_df1 = polarity_df1.append(sia.polarity_scores(i),ignore_index=True)\r\n",
        "data_df_neg = data_df_neg.join(polarity_df1,how=\"right\")\r\n",
        "data_df_neg"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>an attempt at florida film noir , palmetto fai...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv496_11185.txt</td>\n",
              "      <td>-0.5308</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a hotshot lawyer gets an obviously guilty chil...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv265_11625.txt</td>\n",
              "      <td>-0.9941</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chill factor is a carbon copy of speed with on...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv436_20564.txt</td>\n",
              "      <td>-0.7614</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mugshot ( director/writer/cinematographer/edit...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv708_28539.txt</td>\n",
              "      <td>-0.9300</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i never understood what the clich ? \" hell on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv130_18521.txt</td>\n",
              "      <td>-0.9926</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.726</td>\n",
              "      <td>0.089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>from dusk till dawn ( director/editor : robert...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv570_28960.txt</td>\n",
              "      <td>-0.9781</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.822</td>\n",
              "      <td>0.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>annie wilson ( cate blanchett ) , a widow who ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv110_27832.txt</td>\n",
              "      <td>0.9446</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>tommy lee jones chases an innocent victim arou...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv983_24219.txt</td>\n",
              "      <td>0.8970</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>to sum the entire film \" 54 \" up in one senten...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv493_14135.txt</td>\n",
              "      <td>0.9780</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.838</td>\n",
              "      <td>0.111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>i'm going to start this review off with a hypo...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv615_15734.txt</td>\n",
              "      <td>-0.9864</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  data  labels  ...    neu    pos\n",
              "0    an attempt at florida film noir , palmetto fai...       0  ...  0.806  0.092\n",
              "1    a hotshot lawyer gets an obviously guilty chil...       0  ...  0.730  0.116\n",
              "2    chill factor is a carbon copy of speed with on...       0  ...  0.820  0.084\n",
              "3    mugshot ( director/writer/cinematographer/edit...       0  ...  0.794  0.087\n",
              "4    i never understood what the clich ? \" hell on ...       0  ...  0.726  0.089\n",
              "..                                                 ...     ...  ...    ...    ...\n",
              "995  from dusk till dawn ( director/editor : robert...       0  ...  0.822  0.065\n",
              "996  annie wilson ( cate blanchett ) , a widow who ...       0  ...  0.717  0.168\n",
              "997  tommy lee jones chases an innocent victim arou...       0  ...  0.760  0.135\n",
              "998  to sum the entire film \" 54 \" up in one senten...       0  ...  0.838  0.111\n",
              "999  i'm going to start this review off with a hypo...       0  ...  0.806  0.076\n",
              "\n",
              "[1000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FSeZv_Kl1pu"
      },
      "source": [
        "# data_df_neg.iloc[1][0]"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "541XTPgwrBhX",
        "outputId": "46e42c15-77bf-4c40-bbbd-b6eb8090c971"
      },
      "source": [
        "data_df_neg['compound'].describe()"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1000.000000\n",
              "mean        0.105364\n",
              "std         0.902338\n",
              "min        -0.999700\n",
              "25%        -0.953200\n",
              "50%         0.663700\n",
              "75%         0.982100\n",
              "max         0.999600\n",
              "Name: compound, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIhllkJXrFMe"
      },
      "source": [
        "Compound column obtained from polarity scores represents the aggregated sentiment score of the given text. This seemed the most relevant measure for calculating labels. Now, for the threshold, the statistics decribed in the above table shows that the values range from -0.99 (i.e. minimum) to 0.99 (i.e. maximum) with a mean of 0.105. But the median value is 0.663 which implies that the distribution is right skewed and if a value is choosen closer to the mean then we will end up with majority of unmatched document labels. As stated in the question, we need a threshold where *majority of the document labels match*.Choosing the 75th percentile or greater makes more sense as it will include more than 3/4th of the dataset. Also, choosing 0.99 seems like a rather more biased choice which will overestimate the chosen algorithm(here,in SentimentIntensityAnalyzer by Vader). \r\n",
        "\r\n",
        "**Choosen threshold is *0.983* and every value below the threshold will be labeled as *0***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "RQK0rtxtlQSB",
        "outputId": "537c8583-a7d2-47e9-d021-904b8259da40"
      },
      "source": [
        "data_df_neg[data_df_neg['compound']<0.983]"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>an attempt at florida film noir , palmetto fai...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv496_11185.txt</td>\n",
              "      <td>-0.5308</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a hotshot lawyer gets an obviously guilty chil...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv265_11625.txt</td>\n",
              "      <td>-0.9941</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chill factor is a carbon copy of speed with on...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv436_20564.txt</td>\n",
              "      <td>-0.7614</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mugshot ( director/writer/cinematographer/edit...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv708_28539.txt</td>\n",
              "      <td>-0.9300</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i never understood what the clich ? \" hell on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv130_18521.txt</td>\n",
              "      <td>-0.9926</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.726</td>\n",
              "      <td>0.089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>from dusk till dawn ( director/editor : robert...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv570_28960.txt</td>\n",
              "      <td>-0.9781</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.822</td>\n",
              "      <td>0.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>annie wilson ( cate blanchett ) , a widow who ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv110_27832.txt</td>\n",
              "      <td>0.9446</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>tommy lee jones chases an innocent victim arou...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv983_24219.txt</td>\n",
              "      <td>0.8970</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>to sum the entire film \" 54 \" up in one senten...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv493_14135.txt</td>\n",
              "      <td>0.9780</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.838</td>\n",
              "      <td>0.111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>i'm going to start this review off with a hypo...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv615_15734.txt</td>\n",
              "      <td>-0.9864</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>756 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  data  labels  ...    neu    pos\n",
              "0    an attempt at florida film noir , palmetto fai...       0  ...  0.806  0.092\n",
              "1    a hotshot lawyer gets an obviously guilty chil...       0  ...  0.730  0.116\n",
              "2    chill factor is a carbon copy of speed with on...       0  ...  0.820  0.084\n",
              "3    mugshot ( director/writer/cinematographer/edit...       0  ...  0.794  0.087\n",
              "4    i never understood what the clich ? \" hell on ...       0  ...  0.726  0.089\n",
              "..                                                 ...     ...  ...    ...    ...\n",
              "995  from dusk till dawn ( director/editor : robert...       0  ...  0.822  0.065\n",
              "996  annie wilson ( cate blanchett ) , a widow who ...       0  ...  0.717  0.168\n",
              "997  tommy lee jones chases an innocent victim arou...       0  ...  0.760  0.135\n",
              "998  to sum the entire film \" 54 \" up in one senten...       0  ...  0.838  0.111\n",
              "999  i'm going to start this review off with a hypo...       0  ...  0.806  0.076\n",
              "\n",
              "[756 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "5o7eyjO7OuBT",
        "outputId": "d307c298-d1f4-4179-8d94-1da15b27d494"
      },
      "source": [
        "lb = []\r\n",
        "for i in data_df_neg['compound']:\r\n",
        "  if i<0.983:\r\n",
        "    lb.append(0)\r\n",
        "  else:\r\n",
        "    lb.append(1)\r\n",
        "data_df_neg['predicted_labels'] = lb\r\n",
        "data_df_neg"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>predicted_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>an attempt at florida film noir , palmetto fai...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv496_11185.txt</td>\n",
              "      <td>-0.5308</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a hotshot lawyer gets an obviously guilty chil...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv265_11625.txt</td>\n",
              "      <td>-0.9941</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chill factor is a carbon copy of speed with on...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv436_20564.txt</td>\n",
              "      <td>-0.7614</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mugshot ( director/writer/cinematographer/edit...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv708_28539.txt</td>\n",
              "      <td>-0.9300</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i never understood what the clich ? \" hell on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv130_18521.txt</td>\n",
              "      <td>-0.9926</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.726</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>from dusk till dawn ( director/editor : robert...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv570_28960.txt</td>\n",
              "      <td>-0.9781</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.822</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>annie wilson ( cate blanchett ) , a widow who ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv110_27832.txt</td>\n",
              "      <td>0.9446</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>tommy lee jones chases an innocent victim arou...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv983_24219.txt</td>\n",
              "      <td>0.8970</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>to sum the entire film \" 54 \" up in one senten...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv493_14135.txt</td>\n",
              "      <td>0.9780</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.838</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>i'm going to start this review off with a hypo...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv615_15734.txt</td>\n",
              "      <td>-0.9864</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  data  ...  predicted_labels\n",
              "0    an attempt at florida film noir , palmetto fai...  ...                 0\n",
              "1    a hotshot lawyer gets an obviously guilty chil...  ...                 0\n",
              "2    chill factor is a carbon copy of speed with on...  ...                 0\n",
              "3    mugshot ( director/writer/cinematographer/edit...  ...                 0\n",
              "4    i never understood what the clich ? \" hell on ...  ...                 0\n",
              "..                                                 ...  ...               ...\n",
              "995  from dusk till dawn ( director/editor : robert...  ...                 0\n",
              "996  annie wilson ( cate blanchett ) , a widow who ...  ...                 0\n",
              "997  tommy lee jones chases an innocent victim arou...  ...                 0\n",
              "998  to sum the entire film \" 54 \" up in one senten...  ...                 0\n",
              "999  i'm going to start this review off with a hypo...  ...                 0\n",
              "\n",
              "[1000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHiVpWlER0jT",
        "outputId": "042dfb04-b5f5-481d-d077-103e03653727"
      },
      "source": [
        "accuracy = (data_df_neg[data_df_neg['predicted_labels']==data_df_neg['labels']].shape[0]/data_df_neg.shape[0])*100\r\n",
        "print(\"Accuracy percentage is:\")\r\n",
        "print(accuracy)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy percentage is:\n",
            "75.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85QWmwj5izJe"
      },
      "source": [
        "# Positive polarity sentiment scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "DmXRUd9caSX3",
        "outputId": "d6cfe3e8-fb88-4bf6-a054-e275dd2422a9"
      },
      "source": [
        "polarity_df2 = pd.DataFrame()\r\n",
        "for i in data_df_pos.data:\r\n",
        "  polarity_df2 = polarity_df2.append(sia.polarity_scores(i),ignore_index=True)\r\n",
        "data_df_pos = data_df_pos.join(polarity_df2,how=\"right\")\r\n",
        "data_df_pos"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>there seem to be two reactions to dark city . ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv684_11798.txt</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the relaxed dude rides a roller coaster \\nthe ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv990_11591.txt</td>\n",
              "      <td>0.9955</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>weir is well-respected in hollywood for turnin...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv187_12829.txt</td>\n",
              "      <td>0.9957</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>call me crazy , but i don't see saving private...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv441_13711.txt</td>\n",
              "      <td>0.8974</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is it just me , or have disney films gradually...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv967_5788.txt</td>\n",
              "      <td>0.9997</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.734</td>\n",
              "      <td>0.207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>one of the sweetest tales to ever be made , it...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv521_15828.txt</td>\n",
              "      <td>0.9997</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.671</td>\n",
              "      <td>0.249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>after the terminally bleak reservoir dogs and ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv822_20049.txt</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>`run lola run' , a german import that gained a...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv710_22577.txt</td>\n",
              "      <td>0.9965</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.817</td>\n",
              "      <td>0.138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>in october of 1962 the united states found its...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv771_28665.txt</td>\n",
              "      <td>-0.9331</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.771</td>\n",
              "      <td>0.112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>note : some may consider portions of the follo...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv162_10424.txt</td>\n",
              "      <td>-0.9569</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  data  labels  ...    neu    pos\n",
              "0    there seem to be two reactions to dark city . ...       1  ...  0.761  0.162\n",
              "1    the relaxed dude rides a roller coaster \\nthe ...       1  ...  0.827  0.109\n",
              "2    weir is well-respected in hollywood for turnin...       1  ...  0.764  0.174\n",
              "3    call me crazy , but i don't see saving private...       1  ...  0.749  0.132\n",
              "4    is it just me , or have disney films gradually...       1  ...  0.734  0.207\n",
              "..                                                 ...     ...  ...    ...    ...\n",
              "995  one of the sweetest tales to ever be made , it...       1  ...  0.671  0.249\n",
              "996  after the terminally bleak reservoir dogs and ...       1  ...  0.801  0.139\n",
              "997  `run lola run' , a german import that gained a...       1  ...  0.817  0.138\n",
              "998  in october of 1962 the united states found its...       1  ...  0.771  0.112\n",
              "999  note : some may consider portions of the follo...       1  ...  0.802  0.095\n",
              "\n",
              "[1000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4mNGm5BrRw8",
        "outputId": "e78bfec9-836c-440d-bd8d-080bce627c5c"
      },
      "source": [
        "data_df_pos['compound'].describe()"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1000.000000\n",
              "mean        0.647903\n",
              "std         0.700739\n",
              "min        -0.999600\n",
              "25%         0.898600\n",
              "50%         0.990650\n",
              "75%         0.997200\n",
              "max         0.999900\n",
              "Name: compound, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gq7viwuuDa"
      },
      "source": [
        "Compound column describes the aggregated sentiment value of the given text. I want to base the classification, into labels, of the text on \"compound\" polarity score. The stats shown in the above column 3/4th of the dataset lies between 0.99 (maximum value) to 0.898600 (25th percentile). There are comparatively fewer observations/scores that fall under 0.8 ( less that 1/4th). In general, a compound score of 0.8 shows high positive polarity in a dataset although the choice of threshold purely depends on the kind of dataset used. This algorithm performs better for positive scores as opposed to the negative sentiment predictions. Potential reason could be that Vader doesn't penalize against negation that harshly when compared to other algorithms.\r\n",
        "\r\n",
        "**Choosen threshold is *0.8* and every value above the threshold will be labeled as *1***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Am05pbA4VXWO",
        "outputId": "b3ac7910-89dc-498c-b06f-55d9984f7a43"
      },
      "source": [
        "data_df_pos[data_df_pos['compound']>0.8]"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>there seem to be two reactions to dark city . ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv684_11798.txt</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the relaxed dude rides a roller coaster \\nthe ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv990_11591.txt</td>\n",
              "      <td>0.9955</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>weir is well-respected in hollywood for turnin...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv187_12829.txt</td>\n",
              "      <td>0.9957</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>call me crazy , but i don't see saving private...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv441_13711.txt</td>\n",
              "      <td>0.8974</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is it just me , or have disney films gradually...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv967_5788.txt</td>\n",
              "      <td>0.9997</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.734</td>\n",
              "      <td>0.207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>the question isn't why has grease been reissue...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv840_16321.txt</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>ingredients : neophyte lawyer , legal situatio...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv212_10027.txt</td>\n",
              "      <td>0.9935</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.724</td>\n",
              "      <td>0.196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>one of the sweetest tales to ever be made , it...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv521_15828.txt</td>\n",
              "      <td>0.9997</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.671</td>\n",
              "      <td>0.249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>after the terminally bleak reservoir dogs and ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv822_20049.txt</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>`run lola run' , a german import that gained a...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv710_22577.txt</td>\n",
              "      <td>0.9965</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.817</td>\n",
              "      <td>0.138</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>792 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  data  labels  ...    neu    pos\n",
              "0    there seem to be two reactions to dark city . ...       1  ...  0.761  0.162\n",
              "1    the relaxed dude rides a roller coaster \\nthe ...       1  ...  0.827  0.109\n",
              "2    weir is well-respected in hollywood for turnin...       1  ...  0.764  0.174\n",
              "3    call me crazy , but i don't see saving private...       1  ...  0.749  0.132\n",
              "4    is it just me , or have disney films gradually...       1  ...  0.734  0.207\n",
              "..                                                 ...     ...  ...    ...    ...\n",
              "993  the question isn't why has grease been reissue...       1  ...  0.794  0.159\n",
              "994  ingredients : neophyte lawyer , legal situatio...       1  ...  0.724  0.196\n",
              "995  one of the sweetest tales to ever be made , it...       1  ...  0.671  0.249\n",
              "996  after the terminally bleak reservoir dogs and ...       1  ...  0.801  0.139\n",
              "997  `run lola run' , a german import that gained a...       1  ...  0.817  0.138\n",
              "\n",
              "[792 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "bN0lunsoZGVa",
        "outputId": "921dae62-563c-4e1f-9e0a-6e5bec17def5"
      },
      "source": [
        "lb = []\r\n",
        "for i in data_df_pos['compound']:\r\n",
        "  if i>0.8:\r\n",
        "    lb.append(1)\r\n",
        "  else:\r\n",
        "    lb.append(0)\r\n",
        "data_df_pos['predicted_labels'] = lb\r\n",
        "data_df_pos"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>predicted_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>there seem to be two reactions to dark city . ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv684_11798.txt</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.162</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the relaxed dude rides a roller coaster \\nthe ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv990_11591.txt</td>\n",
              "      <td>0.9955</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.109</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>weir is well-respected in hollywood for turnin...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv187_12829.txt</td>\n",
              "      <td>0.9957</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.174</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>call me crazy , but i don't see saving private...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv441_13711.txt</td>\n",
              "      <td>0.8974</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.132</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is it just me , or have disney films gradually...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv967_5788.txt</td>\n",
              "      <td>0.9997</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.734</td>\n",
              "      <td>0.207</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>one of the sweetest tales to ever be made , it...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv521_15828.txt</td>\n",
              "      <td>0.9997</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.671</td>\n",
              "      <td>0.249</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>after the terminally bleak reservoir dogs and ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv822_20049.txt</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.139</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>`run lola run' , a german import that gained a...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv710_22577.txt</td>\n",
              "      <td>0.9965</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.817</td>\n",
              "      <td>0.138</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>in october of 1962 the united states found its...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv771_28665.txt</td>\n",
              "      <td>-0.9331</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.771</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>note : some may consider portions of the follo...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv162_10424.txt</td>\n",
              "      <td>-0.9569</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  data  ...  predicted_labels\n",
              "0    there seem to be two reactions to dark city . ...  ...                 1\n",
              "1    the relaxed dude rides a roller coaster \\nthe ...  ...                 1\n",
              "2    weir is well-respected in hollywood for turnin...  ...                 1\n",
              "3    call me crazy , but i don't see saving private...  ...                 1\n",
              "4    is it just me , or have disney films gradually...  ...                 1\n",
              "..                                                 ...  ...               ...\n",
              "995  one of the sweetest tales to ever be made , it...  ...                 1\n",
              "996  after the terminally bleak reservoir dogs and ...  ...                 1\n",
              "997  `run lola run' , a german import that gained a...  ...                 1\n",
              "998  in october of 1962 the united states found its...  ...                 0\n",
              "999  note : some may consider portions of the follo...  ...                 0\n",
              "\n",
              "[1000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62Kp7XYzZXZ8",
        "outputId": "21c43d09-fb32-48cf-b132-3bcf69ab9256"
      },
      "source": [
        "accuracy = (data_df_pos[data_df_pos['predicted_labels']==data_df_pos['labels']].shape[0]/data_df_pos.shape[0])*100\r\n",
        "print(\"Accuracy percentage is:\")\r\n",
        "print(accuracy)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy percentage is:\n",
            "79.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N-xgimgtzlb"
      },
      "source": [
        "# Bonus \r\n",
        "(40 points): \r\n",
        "Repeat questions 2,3 and 4 removing all stopwords. Answer the\r\n",
        "following questions: Did this change the results in any way? Why do you think so?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5JWE7CUAAdA",
        "outputId": "775c5dfe-9de4-4b6f-dfef-6bd2110fdf69"
      },
      "source": [
        "print(s_words)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "RkRlo5Tfvh30",
        "outputId": "82f0c0a0-572d-466f-882f-1d36317b2fe7"
      },
      "source": [
        "df_no_stopwords = pd.DataFrame()\r\n",
        "w_str=\"\"\r\n",
        "s_count = 0\r\n",
        "r_count = 0\r\n",
        "for index,i in data_df.iterrows():\r\n",
        "  for w in nltk.word_tokenize(i.data):\r\n",
        "    if w not in s_words:\r\n",
        "      w_str += w+\" \"\r\n",
        "      r_count +=1\r\n",
        "    else:\r\n",
        "      s_count += 1\r\n",
        "  df_no_stopwords = df_no_stopwords.append({'data':w_str,'labels':i['labels'],'name':i['name']},ignore_index=True)\r\n",
        "  w_str=\"\"\r\n",
        "df_no_stopwords['labels'] = df_no_stopwords['labels'].astype('int32')\r\n",
        "df_no_stopwords"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>attempt florida film noir , palmetto fails fun...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv496_11185.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hotshot lawyer gets obviously guilty child mol...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv265_11625.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chill factor carbon copy speed one notable exc...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv436_20564.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mugshot ( director/writer/cinematographer/edit...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv708_28539.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>never understood clich ? `` hell earth `` trul...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv130_18521.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>one sweetest tales ever made , 's wonderful li...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv521_15828.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>terminally bleak reservoir dogs brutally viole...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv822_20049.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>`run lola run ' , german import gained stellar...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv710_22577.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>october 1962 united states found brink nuclear...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv771_28665.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>note : may consider portions following text sp...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv162_10424.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   data  ...             name\n",
              "0     attempt florida film noir , palmetto fails fun...  ...  cv496_11185.txt\n",
              "1     hotshot lawyer gets obviously guilty child mol...  ...  cv265_11625.txt\n",
              "2     chill factor carbon copy speed one notable exc...  ...  cv436_20564.txt\n",
              "3     mugshot ( director/writer/cinematographer/edit...  ...  cv708_28539.txt\n",
              "4     never understood clich ? `` hell earth `` trul...  ...  cv130_18521.txt\n",
              "...                                                 ...  ...              ...\n",
              "1995  one sweetest tales ever made , 's wonderful li...  ...  cv521_15828.txt\n",
              "1996  terminally bleak reservoir dogs brutally viole...  ...  cv822_20049.txt\n",
              "1997  `run lola run ' , german import gained stellar...  ...  cv710_22577.txt\n",
              "1998  october 1962 united states found brink nuclear...  ...  cv771_28665.txt\n",
              "1999  note : may consider portions following text sp...  ...  cv162_10424.txt\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toxxPKDt-1g_"
      },
      "source": [
        "# print(df_no_stopwords.iloc[0][0])\r\n",
        "# print(\"\\n\")\r\n",
        "# print(data_df.iloc[0][0])"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDe_9iVCvWng"
      },
      "source": [
        "def split_setStop(pos_per,neg_per):\r\n",
        "  a_train,a_test = train_test_split(df_no_stopwords[df_no_stopwords['labels']==0], test_size=neg_per, random_state=seed_value)\r\n",
        "  a_train1,a_test1 = train_test_split(df_no_stopwords[df_no_stopwords['labels']==1], test_size=pos_per, random_state=seed_value)\r\n",
        "  a_train = a_train.append(a_train1)\r\n",
        "  a_test = a_test.append(a_test1)\r\n",
        "  return a_train,a_test"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1jwZnDMvdad"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\r\n",
        "def train_stop_model(aName,vecName, trainSet):\r\n",
        "  model = make_pipeline(vecName, aName)\r\n",
        "  model.fit(trainSet.data, trainSet.labels)\r\n",
        "  return model"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAEG2OqTCt1r"
      },
      "source": [
        "# 50% of the positive dataset and 70% of the negative dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6z33yQrCt13"
      },
      "source": [
        "train_stop_a,test_stop_a = split_setStop(0.50,0.30)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrsnBkziC8Ao"
      },
      "source": [
        "# NaiveBayes with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIo1HRjSC8Ar"
      },
      "source": [
        "model_stop_a = train_stop_model(MultinomialNB(),TfidfVectorizer(),train_stop_a)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2wX6EPcDc0I"
      },
      "source": [
        "# 50% of the negative dataset and 70% of the positive dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKME8-h-Dc0J"
      },
      "source": [
        "train_stop_b,test_stop_b = split_setStop(0.30,0.50)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO3l4ejcEfTw"
      },
      "source": [
        "# NaiveBayes with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmjUmRB5DR0n"
      },
      "source": [
        "model_stop_b = train_stop_model(MultinomialNB(),TfidfVectorizer(),train_stop_b)"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbEBubrSDmoO"
      },
      "source": [
        "# 25% of the negative dataset and 25% of the positive dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHChRLp5DmoO"
      },
      "source": [
        "train_stop_c,test_stop_c = split_setStop(0.75,0.75)"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5iP6iy6Dsz3"
      },
      "source": [
        "# SVM with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxg9n9HzDsz5"
      },
      "source": [
        "model_stop_c = train_stop_model(SVC(),TfidfVectorizer(),train_stop_c)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhKf-6I8E0dN"
      },
      "source": [
        "# Using the **evaluate_model** method created in Q3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTEiZeTxE57Q"
      },
      "source": [
        "# a) 50% positive and 30% negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPjs7LGeE57S"
      },
      "source": [
        "accuracy,precision,recall,f1score=evaluate_model(test_stop_a,model_stop_a)"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ql7THv7E57U",
        "outputId": "cecae1c9-a5d8-4313-c23c-cc34bc0d55b2"
      },
      "source": [
        "print(\"Model a \\n\")\r\n",
        "print('Accuracy:', accuracy)\r\n",
        "print('Precision:', precision)\r\n",
        "print('Recall:', recall)\r\n",
        "print('F1 Score:', f1score)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model a \n",
            "\n",
            "Accuracy: 0.3975\n",
            "Precision: 1.0\n",
            "Recall: 0.036\n",
            "F1 Score: 0.3120133600725098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suiNfMgYFROu"
      },
      "source": [
        "# b) 50% negative and 30% positive, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGrIpZmiFROu"
      },
      "source": [
        "accuracy,precision,recall,f1score=evaluate_model(test_stop_b,model_stop_b)"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6quZK_84FROu",
        "outputId": "c4667efd-7bc2-4ac0-f7fb-67e0a0164bdf"
      },
      "source": [
        "print(\"Model b \\n\")\r\n",
        "print('Accuracy:', accuracy)\r\n",
        "print('Precision:', precision)\r\n",
        "print('Recall:', recall)\r\n",
        "print('F1 Score:', f1score)"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model b \n",
            "\n",
            "Accuracy: 0.3825\n",
            "Precision: 0.3778337531486146\n",
            "Recall: 1.0\n",
            "F1 Score: 0.2860807422447991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjWPBzwUFdHp"
      },
      "source": [
        "# c) 75% negative and 75% positive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wk57bIQFdHq"
      },
      "source": [
        "accuracy,precision,recall,f1score=evaluate_model(test_stop_c,model_stop_c)"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5Q8FVigFdHq",
        "outputId": "45231c0a-408c-464d-8e77-6b8f7c52fb22"
      },
      "source": [
        "print(\"Model c \\n\")\r\n",
        "print('Accuracy:', accuracy)\r\n",
        "print('Precision:', precision)\r\n",
        "print('Recall:', recall)\r\n",
        "print('F1 Score:', f1score)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model c \n",
            "\n",
            "Accuracy: 0.7553333333333333\n",
            "Precision: 0.7343941248470012\n",
            "Recall: 0.8\n",
            "F1 Score: 0.7548442203133274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hST1TnBGAiT"
      },
      "source": [
        "# Using Vader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqe8PKFxHXaq"
      },
      "source": [
        "sia_stop = SIA()"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGm5evUxGDTQ"
      },
      "source": [
        "def polarity_calc(df_s):\r\n",
        "  polarity_df1 = pd.DataFrame()\r\n",
        "  for i in df_s.data:\r\n",
        "    polarity_df1 = polarity_df1.append(sia_stop.polarity_scores(i),ignore_index=True)\r\n",
        "  df_s = df_s.join(polarity_df1,how=\"right\")\r\n",
        "  return df_s\r\n"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlwmVD-KSYbH"
      },
      "source": [
        "# df_no_stopwords"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaVwIfyAKfnD"
      },
      "source": [
        "df_no_stop_neg = polarity_calc(df_no_stopwords[df_no_stopwords['labels']==0].reset_index(drop=True))\r\n",
        "df_no_stop_pos = polarity_calc(df_no_stopwords[df_no_stopwords['labels']==1].reset_index(drop=True))"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "7XDk1ocdU6P9",
        "outputId": "a4821d73-3d0b-4c12-90e8-07a5098bb8e0"
      },
      "source": [
        "df_no_stop_neg.describe()"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.178472</td>\n",
              "      <td>0.151750</td>\n",
              "      <td>0.673120</td>\n",
              "      <td>0.175106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.884680</td>\n",
              "      <td>0.050057</td>\n",
              "      <td>0.056097</td>\n",
              "      <td>0.045794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.998900</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.431000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.914775</td>\n",
              "      <td>0.118000</td>\n",
              "      <td>0.636000</td>\n",
              "      <td>0.142750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796250</td>\n",
              "      <td>0.148000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.173000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.978725</td>\n",
              "      <td>0.183250</td>\n",
              "      <td>0.710000</td>\n",
              "      <td>0.202000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.998900</td>\n",
              "      <td>0.569000</td>\n",
              "      <td>0.894000</td>\n",
              "      <td>0.345000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       labels     compound          neg          neu          pos\n",
              "count  1000.0  1000.000000  1000.000000  1000.000000  1000.000000\n",
              "mean      0.0     0.178472     0.151750     0.673120     0.175106\n",
              "std       0.0     0.884680     0.050057     0.056097     0.045794\n",
              "min       0.0    -0.998900     0.030000     0.431000     0.000000\n",
              "25%       0.0    -0.914775     0.118000     0.636000     0.142750\n",
              "50%       0.0     0.796250     0.148000     0.675000     0.173000\n",
              "75%       0.0     0.978725     0.183250     0.710000     0.202000\n",
              "max       0.0     0.998900     0.569000     0.894000     0.345000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "PRLnJvhoHCyQ",
        "outputId": "b34c3613-5cc4-4b36-9269-57ec28fd86d5"
      },
      "source": [
        "df_no_stop_pos.describe()"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.635817</td>\n",
              "      <td>0.122094</td>\n",
              "      <td>0.670657</td>\n",
              "      <td>0.207236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.700532</td>\n",
              "      <td>0.049264</td>\n",
              "      <td>0.056819</td>\n",
              "      <td>0.053313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.999200</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>0.066000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.899200</td>\n",
              "      <td>0.087000</td>\n",
              "      <td>0.634000</td>\n",
              "      <td>0.168000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.987100</td>\n",
              "      <td>0.116000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.204000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996125</td>\n",
              "      <td>0.152000</td>\n",
              "      <td>0.707000</td>\n",
              "      <td>0.238000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.999800</td>\n",
              "      <td>0.314000</td>\n",
              "      <td>0.838000</td>\n",
              "      <td>0.418000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       labels     compound          neg          neu          pos\n",
              "count  1000.0  1000.000000  1000.000000  1000.000000  1000.000000\n",
              "mean      1.0     0.635817     0.122094     0.670657     0.207236\n",
              "std       0.0     0.700532     0.049264     0.056819     0.053313\n",
              "min       1.0    -0.999200     0.010000     0.449000     0.066000\n",
              "25%       1.0     0.899200     0.087000     0.634000     0.168000\n",
              "50%       1.0     0.987100     0.116000     0.673000     0.204000\n",
              "75%       1.0     0.996125     0.152000     0.707000     0.238000\n",
              "max       1.0     0.999800     0.314000     0.838000     0.418000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDMM3verHBrb"
      },
      "source": [
        ""
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB3E7RMPGV0F"
      },
      "source": [
        "def predict_labels_stop(df,theta,check):\r\n",
        "  lb = []\r\n",
        "  for i in df['compound']:\r\n",
        "    if check == \">\":\r\n",
        "      if i>theta:\r\n",
        "        lb.append(1)\r\n",
        "      else:\r\n",
        "        lb.append(0)\r\n",
        "    elif check == \"<\":\r\n",
        "      if i<theta:\r\n",
        "        lb.append(0)\r\n",
        "      else:\r\n",
        "        lb.append(1)\r\n",
        "  df['predicted_labels'] = lb\r\n",
        "  return df"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "_KM7oP1PKPME",
        "outputId": "47d1c40e-210a-411e-e3c0-06d1b3ede24a"
      },
      "source": [
        "pred_spos_df = predict_labels_stop(df_no_stop_pos,0.85,\">\")\r\n",
        "pred_spos_df"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>predicted_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>seem two reactions dark city . either love uni...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv684_11798.txt</td>\n",
              "      <td>0.9946</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.671</td>\n",
              "      <td>0.232</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>relaxed dude rides roller coaster big lebowski...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv990_11591.txt</td>\n",
              "      <td>0.9934</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.776</td>\n",
              "      <td>0.144</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>weir well-respected hollywood turning scripts ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv187_12829.txt</td>\n",
              "      <td>0.9954</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.257</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>call crazy , n't see saving private ryan film ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv441_13711.txt</td>\n",
              "      <td>-0.4179</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.708</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>, disney films gradually lost appeal ? almost ...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv967_5788.txt</td>\n",
              "      <td>0.9995</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.663</td>\n",
              "      <td>0.274</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>one sweetest tales ever made , 's wonderful li...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv521_15828.txt</td>\n",
              "      <td>0.9992</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.598</td>\n",
              "      <td>0.300</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>terminally bleak reservoir dogs brutally viole...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv822_20049.txt</td>\n",
              "      <td>0.9905</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.173</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>`run lola run ' , german import gained stellar...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv710_22577.txt</td>\n",
              "      <td>0.9952</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.190</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>october 1962 united states found brink nuclear...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv771_28665.txt</td>\n",
              "      <td>-0.9779</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.679</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>note : may consider portions following text sp...</td>\n",
              "      <td>1</td>\n",
              "      <td>cv162_10424.txt</td>\n",
              "      <td>-0.5160</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.712</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  data  ...  predicted_labels\n",
              "0    seem two reactions dark city . either love uni...  ...                 1\n",
              "1    relaxed dude rides roller coaster big lebowski...  ...                 1\n",
              "2    weir well-respected hollywood turning scripts ...  ...                 1\n",
              "3    call crazy , n't see saving private ryan film ...  ...                 0\n",
              "4    , disney films gradually lost appeal ? almost ...  ...                 1\n",
              "..                                                 ...  ...               ...\n",
              "995  one sweetest tales ever made , 's wonderful li...  ...                 1\n",
              "996  terminally bleak reservoir dogs brutally viole...  ...                 1\n",
              "997  `run lola run ' , german import gained stellar...  ...                 1\n",
              "998  october 1962 united states found brink nuclear...  ...                 0\n",
              "999  note : may consider portions following text sp...  ...                 0\n",
              "\n",
              "[1000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlhEF165WCcN",
        "outputId": "44465aaf-a094-4ad0-8691-bc396acd72b4"
      },
      "source": [
        "accuracy = (pred_spos_df[pred_spos_df['predicted_labels']==pred_spos_df['labels']].shape[0]/pred_spos_df.shape[0])*100\r\n",
        "print(\"Accuracy percentage is:\")\r\n",
        "print(accuracy)"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy percentage is:\n",
            "76.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Vai0a90hWLW8",
        "outputId": "b87a6607-1d9f-49bd-fd1f-724d70007899"
      },
      "source": [
        "pred_sneg_df = predict_labels_stop(df_no_stop_neg,0.979,\"<\")\r\n",
        "pred_sneg_df"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "      <th>name</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>predicted_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>attempt florida film noir , palmetto fails fun...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv496_11185.txt</td>\n",
              "      <td>-0.9050</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hotshot lawyer gets obviously guilty child mol...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv265_11625.txt</td>\n",
              "      <td>-0.9637</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.650</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chill factor carbon copy speed one notable exc...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv436_20564.txt</td>\n",
              "      <td>-0.8543</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.692</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mugshot ( director/writer/cinematographer/edit...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv708_28539.txt</td>\n",
              "      <td>-0.9319</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.720</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>never understood clich ? `` hell earth `` trul...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv130_18521.txt</td>\n",
              "      <td>-0.9462</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.610</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>dusk till dawn ( director/editor : robert rodr...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv570_28960.txt</td>\n",
              "      <td>-0.9962</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.658</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>annie wilson ( cate blanchett ) , widow strugg...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv110_27832.txt</td>\n",
              "      <td>0.8538</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>tommy lee jones chases innocent victim around ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv983_24219.txt</td>\n",
              "      <td>-0.8265</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.601</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>sum entire film `` 54 `` one sentence , would ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv493_14135.txt</td>\n",
              "      <td>0.9918</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.740</td>\n",
              "      <td>0.192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>'m going start review hypothetical question . ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cv615_15734.txt</td>\n",
              "      <td>-0.9729</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  data  ...  predicted_labels\n",
              "0    attempt florida film noir , palmetto fails fun...  ...                 0\n",
              "1    hotshot lawyer gets obviously guilty child mol...  ...                 0\n",
              "2    chill factor carbon copy speed one notable exc...  ...                 0\n",
              "3    mugshot ( director/writer/cinematographer/edit...  ...                 0\n",
              "4    never understood clich ? `` hell earth `` trul...  ...                 0\n",
              "..                                                 ...  ...               ...\n",
              "995  dusk till dawn ( director/editor : robert rodr...  ...                 0\n",
              "996  annie wilson ( cate blanchett ) , widow strugg...  ...                 0\n",
              "997  tommy lee jones chases innocent victim around ...  ...                 0\n",
              "998  sum entire film `` 54 `` one sentence , would ...  ...                 1\n",
              "999  'm going start review hypothetical question . ...  ...                 0\n",
              "\n",
              "[1000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqpryuTiWde6",
        "outputId": "9acd7651-19e0-46fa-bc71-579c000baa1e"
      },
      "source": [
        "accuracy = (pred_sneg_df[pred_sneg_df['predicted_labels']==pred_sneg_df['labels']].shape[0]/pred_sneg_df.shape[0])*100\r\n",
        "print(\"Accuracy percentage is:\")\r\n",
        "print(accuracy)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy percentage is:\n",
            "75.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxAb_BAamj_c",
        "outputId": "24278d26-a682-47b1-fb13-ce2d8e9ed6d9"
      },
      "source": [
        "print(\"Total stop words:\")\r\n",
        "print(s_count)\r\n",
        "print(\"Total words that are not stop words:\")\r\n",
        "print(r_count)\r\n",
        "print(\"Percentage of stopwords:\")\r\n",
        "print(s_count*100/(s_count+r_count))\r\n",
        "print(\"Percentage of non stopwords:\")\r\n",
        "print(r_count*100/(s_count+r_count))"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total stop words:\n",
            "595282\n",
            "Total words that are not stop words:\n",
            "929208\n",
            "Percentage of stopwords:\n",
            "39.04794390255102\n",
            "Percentage of non stopwords:\n",
            "60.95205609744898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRHn3sBNkF4r"
      },
      "source": [
        "There is no significant difference between before stop words removal and after stop removal versions of models in Q2. There is a slight 0.01% increase of accuracy of models that are trained after stop words removal. The kind of dataset used probably plays the biggest role in determining the observed effects. One potential reason could be, stop words vary from document to document as what type of dataset it is. Dataset for movies reviews might contain words like film, movie,etc. which add no new information to the analysis. It is important to find such words with a clear analysis and then apply something like stopword removal than just applying a general pool of stop words. Another reason also could be about the size of dataset used and the number of rare words vs common words in the document. Overall understanding is that the Naive Bayes yielded slightly more better result after stopword removal than the SVM.\r\n",
        "\r\n",
        "For Vader, it is observed that the accuracy has declined by over 2% for positive labels prediction for the choosen threshold which were based on the 25th percentile value of the compounded polarity score. Also, the chosen threshold for both is based on their individual statistics. For negative labels the accuracy has declined insignificantly by 0.6% which does not convey much about how helpful was the stop word removal. Rather, it seemed that stop word removal caused a negative impact on Vader. One potential reason could be that the removal of stopwords like but, then etc. resulted in misinterpretation of the context as Vader not just looks at the frequency of words vs its lexicons but also considers the context and the style of writing."
      ]
    }
  ]
}