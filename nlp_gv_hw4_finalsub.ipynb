{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_gv_hw4_finalsub.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4NT52LNyGXT7Z3tO4Fs7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varchala/NaturalLanguageProcessing_CSC8980/blob/main/nlp_gv_hw4_finalsub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb4goKtRd2EW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lxxO2dYcfGf"
      },
      "source": [
        "G VARCHALESWARI\n",
        "vganugapati1@student.gsu.edu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e3txwbh3q76"
      },
      "source": [
        "import nltk\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import  RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "class_names = [\"Negative\", \"Positive\"]\n",
        "seed_value=2361"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apEzs3-3lPQb"
      },
      "source": [
        "train = pd.read_csv(\"/content/sample_data/train.csv\",encoding = \"ISO-8859-1\")\n",
        "# test = pd.read_csv(\"/content/sample_data/test.csv\",encoding = \"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkUbq4y2REyI"
      },
      "source": [
        "# **1.**\n",
        "Take the positive and the negative tweets only. Use Sklearn to split the dataset in 80%\n",
        "training, 20% testing splits. Provide a nicely formatted summary of these splits,\n",
        "containing their size) (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9yZXzEKRNUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "dadf4b6d-7bd8-4622-fb9e-c95eaaeac3b3"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1         0  ...  is upset that he can't update his Facebook by ...\n",
              "2         0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3         0  ...    my whole body feels itchy and like its on fire \n",
              "4         0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXJoFSZFyrc0"
      },
      "source": [
        "a_train,a_test = train_test_split(train[(train['polarity']==0) | (train['polarity']==4)], test_size=0.20, random_state=seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofb7GXYRpfMH",
        "outputId": "decefe8d-6bc9-4274-ef25-c066c1e971b2"
      },
      "source": [
        "print(\"Training set summary:\\n\")\n",
        "print(a_train.info())\n",
        "print(a_train.describe())\n",
        "print(\"\\nTest set summary:\\n\")\n",
        "print(a_test.info())\n",
        "print(a_test.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set summary:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1280000 entries, 1474686 to 1561195\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count    Dtype \n",
            "---  ------    --------------    ----- \n",
            " 0   polarity  1280000 non-null  int64 \n",
            " 1   id        1280000 non-null  int64 \n",
            " 2   date      1280000 non-null  object\n",
            " 3   query     1280000 non-null  object\n",
            " 4   user      1280000 non-null  object\n",
            " 5   text      1280000 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 68.4+ MB\n",
            "None\n",
            "           polarity            id\n",
            "count  1.280000e+06  1.280000e+06\n",
            "mean   1.999994e+00  1.998838e+09\n",
            "std    2.000001e+00  1.935154e+08\n",
            "min    0.000000e+00  1.467810e+09\n",
            "25%    0.000000e+00  1.956901e+09\n",
            "50%    0.000000e+00  2.002074e+09\n",
            "75%    4.000000e+00  2.177051e+09\n",
            "max    4.000000e+00  2.329206e+09\n",
            "\n",
            "Test set summary:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 320000 entries, 402563 to 1311901\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count   Dtype \n",
            "---  ------    --------------   ----- \n",
            " 0   polarity  320000 non-null  int64 \n",
            " 1   id        320000 non-null  int64 \n",
            " 2   date      320000 non-null  object\n",
            " 3   query     320000 non-null  object\n",
            " 4   user      320000 non-null  object\n",
            " 5   text      320000 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 17.1+ MB\n",
            "None\n",
            "            polarity            id\n",
            "count  320000.000000  3.200000e+05\n",
            "mean        2.000025  1.998736e+09\n",
            "std         2.000003  1.938190e+08\n",
            "min         0.000000  1.467814e+09\n",
            "25%         0.000000  1.956980e+09\n",
            "50%         4.000000  2.002215e+09\n",
            "75%         4.000000  2.177101e+09\n",
            "max         4.000000  2.329206e+09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcHqpFQwra3f"
      },
      "source": [
        "# **2.**\n",
        "Use the code from the previous classes to build the following models (15 points):\n",
        "\n",
        "a) SVM using TF-IDF.\n",
        "\n",
        "b) Naive Bayes using TF-IDF.\n",
        "\n",
        "c) Random Forest using TF-IDF.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVlWu0Y8Xurg"
      },
      "source": [
        "def train_model(aName,vecName, x,y):\n",
        "  model = aName\n",
        "  batch = 1\n",
        "  for i in range(0,x.shape[0],1000):\n",
        "    print(\"Training batch {}...\".format(batch))\n",
        "    batch += 1\n",
        "    model.partial_fit(x[i:i+1000,:].todense().astype(np.float16), y[i:i+1000],classes=np.unique(y))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7iGusgAw3zS"
      },
      "source": [
        "# trans_data = trans_data.astype(np.float16)\n",
        "vec = TfidfVectorizer()\n",
        "x_train = vec.fit_transform(a_train.text)\n",
        "y_train = a_train.polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdBNTvD6Q4NS"
      },
      "source": [
        "x_test = vec.transform(a_test.text.values)\n",
        "y_test = a_test.polarity.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZBZ-2WIbe-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7814d9b1-813a-4854-c3f7-60dadc127b46"
      },
      "source": [
        "model_nb = train_model(MultinomialNB(),TfidfVectorizer(),x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training batch 1...\n",
            "Training batch 2...\n",
            "Training batch 3...\n",
            "Training batch 4...\n",
            "Training batch 5...\n",
            "Training batch 6...\n",
            "Training batch 7...\n",
            "Training batch 8...\n",
            "Training batch 9...\n",
            "Training batch 10...\n",
            "Training batch 11...\n",
            "Training batch 12...\n",
            "Training batch 13...\n",
            "Training batch 14...\n",
            "Training batch 15...\n",
            "Training batch 16...\n",
            "Training batch 17...\n",
            "Training batch 18...\n",
            "Training batch 19...\n",
            "Training batch 20...\n",
            "Training batch 21...\n",
            "Training batch 22...\n",
            "Training batch 23...\n",
            "Training batch 24...\n",
            "Training batch 25...\n",
            "Training batch 26...\n",
            "Training batch 27...\n",
            "Training batch 28...\n",
            "Training batch 29...\n",
            "Training batch 30...\n",
            "Training batch 31...\n",
            "Training batch 32...\n",
            "Training batch 33...\n",
            "Training batch 34...\n",
            "Training batch 35...\n",
            "Training batch 36...\n",
            "Training batch 37...\n",
            "Training batch 38...\n",
            "Training batch 39...\n",
            "Training batch 40...\n",
            "Training batch 41...\n",
            "Training batch 42...\n",
            "Training batch 43...\n",
            "Training batch 44...\n",
            "Training batch 45...\n",
            "Training batch 46...\n",
            "Training batch 47...\n",
            "Training batch 48...\n",
            "Training batch 49...\n",
            "Training batch 50...\n",
            "Training batch 51...\n",
            "Training batch 52...\n",
            "Training batch 53...\n",
            "Training batch 54...\n",
            "Training batch 55...\n",
            "Training batch 56...\n",
            "Training batch 57...\n",
            "Training batch 58...\n",
            "Training batch 59...\n",
            "Training batch 60...\n",
            "Training batch 61...\n",
            "Training batch 62...\n",
            "Training batch 63...\n",
            "Training batch 64...\n",
            "Training batch 65...\n",
            "Training batch 66...\n",
            "Training batch 67...\n",
            "Training batch 68...\n",
            "Training batch 69...\n",
            "Training batch 70...\n",
            "Training batch 71...\n",
            "Training batch 72...\n",
            "Training batch 73...\n",
            "Training batch 74...\n",
            "Training batch 75...\n",
            "Training batch 76...\n",
            "Training batch 77...\n",
            "Training batch 78...\n",
            "Training batch 79...\n",
            "Training batch 80...\n",
            "Training batch 81...\n",
            "Training batch 82...\n",
            "Training batch 83...\n",
            "Training batch 84...\n",
            "Training batch 85...\n",
            "Training batch 86...\n",
            "Training batch 87...\n",
            "Training batch 88...\n",
            "Training batch 89...\n",
            "Training batch 90...\n",
            "Training batch 91...\n",
            "Training batch 92...\n",
            "Training batch 93...\n",
            "Training batch 94...\n",
            "Training batch 95...\n",
            "Training batch 96...\n",
            "Training batch 97...\n",
            "Training batch 98...\n",
            "Training batch 99...\n",
            "Training batch 100...\n",
            "Training batch 101...\n",
            "Training batch 102...\n",
            "Training batch 103...\n",
            "Training batch 104...\n",
            "Training batch 105...\n",
            "Training batch 106...\n",
            "Training batch 107...\n",
            "Training batch 108...\n",
            "Training batch 109...\n",
            "Training batch 110...\n",
            "Training batch 111...\n",
            "Training batch 112...\n",
            "Training batch 113...\n",
            "Training batch 114...\n",
            "Training batch 115...\n",
            "Training batch 116...\n",
            "Training batch 117...\n",
            "Training batch 118...\n",
            "Training batch 119...\n",
            "Training batch 120...\n",
            "Training batch 121...\n",
            "Training batch 122...\n",
            "Training batch 123...\n",
            "Training batch 124...\n",
            "Training batch 125...\n",
            "Training batch 126...\n",
            "Training batch 127...\n",
            "Training batch 128...\n",
            "Training batch 129...\n",
            "Training batch 130...\n",
            "Training batch 131...\n",
            "Training batch 132...\n",
            "Training batch 133...\n",
            "Training batch 134...\n",
            "Training batch 135...\n",
            "Training batch 136...\n",
            "Training batch 137...\n",
            "Training batch 138...\n",
            "Training batch 139...\n",
            "Training batch 140...\n",
            "Training batch 141...\n",
            "Training batch 142...\n",
            "Training batch 143...\n",
            "Training batch 144...\n",
            "Training batch 145...\n",
            "Training batch 146...\n",
            "Training batch 147...\n",
            "Training batch 148...\n",
            "Training batch 149...\n",
            "Training batch 150...\n",
            "Training batch 151...\n",
            "Training batch 152...\n",
            "Training batch 153...\n",
            "Training batch 154...\n",
            "Training batch 155...\n",
            "Training batch 156...\n",
            "Training batch 157...\n",
            "Training batch 158...\n",
            "Training batch 159...\n",
            "Training batch 160...\n",
            "Training batch 161...\n",
            "Training batch 162...\n",
            "Training batch 163...\n",
            "Training batch 164...\n",
            "Training batch 165...\n",
            "Training batch 166...\n",
            "Training batch 167...\n",
            "Training batch 168...\n",
            "Training batch 169...\n",
            "Training batch 170...\n",
            "Training batch 171...\n",
            "Training batch 172...\n",
            "Training batch 173...\n",
            "Training batch 174...\n",
            "Training batch 175...\n",
            "Training batch 176...\n",
            "Training batch 177...\n",
            "Training batch 178...\n",
            "Training batch 179...\n",
            "Training batch 180...\n",
            "Training batch 181...\n",
            "Training batch 182...\n",
            "Training batch 183...\n",
            "Training batch 184...\n",
            "Training batch 185...\n",
            "Training batch 186...\n",
            "Training batch 187...\n",
            "Training batch 188...\n",
            "Training batch 189...\n",
            "Training batch 190...\n",
            "Training batch 191...\n",
            "Training batch 192...\n",
            "Training batch 193...\n",
            "Training batch 194...\n",
            "Training batch 195...\n",
            "Training batch 196...\n",
            "Training batch 197...\n",
            "Training batch 198...\n",
            "Training batch 199...\n",
            "Training batch 200...\n",
            "Training batch 201...\n",
            "Training batch 202...\n",
            "Training batch 203...\n",
            "Training batch 204...\n",
            "Training batch 205...\n",
            "Training batch 206...\n",
            "Training batch 207...\n",
            "Training batch 208...\n",
            "Training batch 209...\n",
            "Training batch 210...\n",
            "Training batch 211...\n",
            "Training batch 212...\n",
            "Training batch 213...\n",
            "Training batch 214...\n",
            "Training batch 215...\n",
            "Training batch 216...\n",
            "Training batch 217...\n",
            "Training batch 218...\n",
            "Training batch 219...\n",
            "Training batch 220...\n",
            "Training batch 221...\n",
            "Training batch 222...\n",
            "Training batch 223...\n",
            "Training batch 224...\n",
            "Training batch 225...\n",
            "Training batch 226...\n",
            "Training batch 227...\n",
            "Training batch 228...\n",
            "Training batch 229...\n",
            "Training batch 230...\n",
            "Training batch 231...\n",
            "Training batch 232...\n",
            "Training batch 233...\n",
            "Training batch 234...\n",
            "Training batch 235...\n",
            "Training batch 236...\n",
            "Training batch 237...\n",
            "Training batch 238...\n",
            "Training batch 239...\n",
            "Training batch 240...\n",
            "Training batch 241...\n",
            "Training batch 242...\n",
            "Training batch 243...\n",
            "Training batch 244...\n",
            "Training batch 245...\n",
            "Training batch 246...\n",
            "Training batch 247...\n",
            "Training batch 248...\n",
            "Training batch 249...\n",
            "Training batch 250...\n",
            "Training batch 251...\n",
            "Training batch 252...\n",
            "Training batch 253...\n",
            "Training batch 254...\n",
            "Training batch 255...\n",
            "Training batch 256...\n",
            "Training batch 257...\n",
            "Training batch 258...\n",
            "Training batch 259...\n",
            "Training batch 260...\n",
            "Training batch 261...\n",
            "Training batch 262...\n",
            "Training batch 263...\n",
            "Training batch 264...\n",
            "Training batch 265...\n",
            "Training batch 266...\n",
            "Training batch 267...\n",
            "Training batch 268...\n",
            "Training batch 269...\n",
            "Training batch 270...\n",
            "Training batch 271...\n",
            "Training batch 272...\n",
            "Training batch 273...\n",
            "Training batch 274...\n",
            "Training batch 275...\n",
            "Training batch 276...\n",
            "Training batch 277...\n",
            "Training batch 278...\n",
            "Training batch 279...\n",
            "Training batch 280...\n",
            "Training batch 281...\n",
            "Training batch 282...\n",
            "Training batch 283...\n",
            "Training batch 284...\n",
            "Training batch 285...\n",
            "Training batch 286...\n",
            "Training batch 287...\n",
            "Training batch 288...\n",
            "Training batch 289...\n",
            "Training batch 290...\n",
            "Training batch 291...\n",
            "Training batch 292...\n",
            "Training batch 293...\n",
            "Training batch 294...\n",
            "Training batch 295...\n",
            "Training batch 296...\n",
            "Training batch 297...\n",
            "Training batch 298...\n",
            "Training batch 299...\n",
            "Training batch 300...\n",
            "Training batch 301...\n",
            "Training batch 302...\n",
            "Training batch 303...\n",
            "Training batch 304...\n",
            "Training batch 305...\n",
            "Training batch 306...\n",
            "Training batch 307...\n",
            "Training batch 308...\n",
            "Training batch 309...\n",
            "Training batch 310...\n",
            "Training batch 311...\n",
            "Training batch 312...\n",
            "Training batch 313...\n",
            "Training batch 314...\n",
            "Training batch 315...\n",
            "Training batch 316...\n",
            "Training batch 317...\n",
            "Training batch 318...\n",
            "Training batch 319...\n",
            "Training batch 320...\n",
            "Training batch 321...\n",
            "Training batch 322...\n",
            "Training batch 323...\n",
            "Training batch 324...\n",
            "Training batch 325...\n",
            "Training batch 326...\n",
            "Training batch 327...\n",
            "Training batch 328...\n",
            "Training batch 329...\n",
            "Training batch 330...\n",
            "Training batch 331...\n",
            "Training batch 332...\n",
            "Training batch 333...\n",
            "Training batch 334...\n",
            "Training batch 335...\n",
            "Training batch 336...\n",
            "Training batch 337...\n",
            "Training batch 338...\n",
            "Training batch 339...\n",
            "Training batch 340...\n",
            "Training batch 341...\n",
            "Training batch 342...\n",
            "Training batch 343...\n",
            "Training batch 344...\n",
            "Training batch 345...\n",
            "Training batch 346...\n",
            "Training batch 347...\n",
            "Training batch 348...\n",
            "Training batch 349...\n",
            "Training batch 350...\n",
            "Training batch 351...\n",
            "Training batch 352...\n",
            "Training batch 353...\n",
            "Training batch 354...\n",
            "Training batch 355...\n",
            "Training batch 356...\n",
            "Training batch 357...\n",
            "Training batch 358...\n",
            "Training batch 359...\n",
            "Training batch 360...\n",
            "Training batch 361...\n",
            "Training batch 362...\n",
            "Training batch 363...\n",
            "Training batch 364...\n",
            "Training batch 365...\n",
            "Training batch 366...\n",
            "Training batch 367...\n",
            "Training batch 368...\n",
            "Training batch 369...\n",
            "Training batch 370...\n",
            "Training batch 371...\n",
            "Training batch 372...\n",
            "Training batch 373...\n",
            "Training batch 374...\n",
            "Training batch 375...\n",
            "Training batch 376...\n",
            "Training batch 377...\n",
            "Training batch 378...\n",
            "Training batch 379...\n",
            "Training batch 380...\n",
            "Training batch 381...\n",
            "Training batch 382...\n",
            "Training batch 383...\n",
            "Training batch 384...\n",
            "Training batch 385...\n",
            "Training batch 386...\n",
            "Training batch 387...\n",
            "Training batch 388...\n",
            "Training batch 389...\n",
            "Training batch 390...\n",
            "Training batch 391...\n",
            "Training batch 392...\n",
            "Training batch 393...\n",
            "Training batch 394...\n",
            "Training batch 395...\n",
            "Training batch 396...\n",
            "Training batch 397...\n",
            "Training batch 398...\n",
            "Training batch 399...\n",
            "Training batch 400...\n",
            "Training batch 401...\n",
            "Training batch 402...\n",
            "Training batch 403...\n",
            "Training batch 404...\n",
            "Training batch 405...\n",
            "Training batch 406...\n",
            "Training batch 407...\n",
            "Training batch 408...\n",
            "Training batch 409...\n",
            "Training batch 410...\n",
            "Training batch 411...\n",
            "Training batch 412...\n",
            "Training batch 413...\n",
            "Training batch 414...\n",
            "Training batch 415...\n",
            "Training batch 416...\n",
            "Training batch 417...\n",
            "Training batch 418...\n",
            "Training batch 419...\n",
            "Training batch 420...\n",
            "Training batch 421...\n",
            "Training batch 422...\n",
            "Training batch 423...\n",
            "Training batch 424...\n",
            "Training batch 425...\n",
            "Training batch 426...\n",
            "Training batch 427...\n",
            "Training batch 428...\n",
            "Training batch 429...\n",
            "Training batch 430...\n",
            "Training batch 431...\n",
            "Training batch 432...\n",
            "Training batch 433...\n",
            "Training batch 434...\n",
            "Training batch 435...\n",
            "Training batch 436...\n",
            "Training batch 437...\n",
            "Training batch 438...\n",
            "Training batch 439...\n",
            "Training batch 440...\n",
            "Training batch 441...\n",
            "Training batch 442...\n",
            "Training batch 443...\n",
            "Training batch 444...\n",
            "Training batch 445...\n",
            "Training batch 446...\n",
            "Training batch 447...\n",
            "Training batch 448...\n",
            "Training batch 449...\n",
            "Training batch 450...\n",
            "Training batch 451...\n",
            "Training batch 452...\n",
            "Training batch 453...\n",
            "Training batch 454...\n",
            "Training batch 455...\n",
            "Training batch 456...\n",
            "Training batch 457...\n",
            "Training batch 458...\n",
            "Training batch 459...\n",
            "Training batch 460...\n",
            "Training batch 461...\n",
            "Training batch 462...\n",
            "Training batch 463...\n",
            "Training batch 464...\n",
            "Training batch 465...\n",
            "Training batch 466...\n",
            "Training batch 467...\n",
            "Training batch 468...\n",
            "Training batch 469...\n",
            "Training batch 470...\n",
            "Training batch 471...\n",
            "Training batch 472...\n",
            "Training batch 473...\n",
            "Training batch 474...\n",
            "Training batch 475...\n",
            "Training batch 476...\n",
            "Training batch 477...\n",
            "Training batch 478...\n",
            "Training batch 479...\n",
            "Training batch 480...\n",
            "Training batch 481...\n",
            "Training batch 482...\n",
            "Training batch 483...\n",
            "Training batch 484...\n",
            "Training batch 485...\n",
            "Training batch 486...\n",
            "Training batch 487...\n",
            "Training batch 488...\n",
            "Training batch 489...\n",
            "Training batch 490...\n",
            "Training batch 491...\n",
            "Training batch 492...\n",
            "Training batch 493...\n",
            "Training batch 494...\n",
            "Training batch 495...\n",
            "Training batch 496...\n",
            "Training batch 497...\n",
            "Training batch 498...\n",
            "Training batch 499...\n",
            "Training batch 500...\n",
            "Training batch 501...\n",
            "Training batch 502...\n",
            "Training batch 503...\n",
            "Training batch 504...\n",
            "Training batch 505...\n",
            "Training batch 506...\n",
            "Training batch 507...\n",
            "Training batch 508...\n",
            "Training batch 509...\n",
            "Training batch 510...\n",
            "Training batch 511...\n",
            "Training batch 512...\n",
            "Training batch 513...\n",
            "Training batch 514...\n",
            "Training batch 515...\n",
            "Training batch 516...\n",
            "Training batch 517...\n",
            "Training batch 518...\n",
            "Training batch 519...\n",
            "Training batch 520...\n",
            "Training batch 521...\n",
            "Training batch 522...\n",
            "Training batch 523...\n",
            "Training batch 524...\n",
            "Training batch 525...\n",
            "Training batch 526...\n",
            "Training batch 527...\n",
            "Training batch 528...\n",
            "Training batch 529...\n",
            "Training batch 530...\n",
            "Training batch 531...\n",
            "Training batch 532...\n",
            "Training batch 533...\n",
            "Training batch 534...\n",
            "Training batch 535...\n",
            "Training batch 536...\n",
            "Training batch 537...\n",
            "Training batch 538...\n",
            "Training batch 539...\n",
            "Training batch 540...\n",
            "Training batch 541...\n",
            "Training batch 542...\n",
            "Training batch 543...\n",
            "Training batch 544...\n",
            "Training batch 545...\n",
            "Training batch 546...\n",
            "Training batch 547...\n",
            "Training batch 548...\n",
            "Training batch 549...\n",
            "Training batch 550...\n",
            "Training batch 551...\n",
            "Training batch 552...\n",
            "Training batch 553...\n",
            "Training batch 554...\n",
            "Training batch 555...\n",
            "Training batch 556...\n",
            "Training batch 557...\n",
            "Training batch 558...\n",
            "Training batch 559...\n",
            "Training batch 560...\n",
            "Training batch 561...\n",
            "Training batch 562...\n",
            "Training batch 563...\n",
            "Training batch 564...\n",
            "Training batch 565...\n",
            "Training batch 566...\n",
            "Training batch 567...\n",
            "Training batch 568...\n",
            "Training batch 569...\n",
            "Training batch 570...\n",
            "Training batch 571...\n",
            "Training batch 572...\n",
            "Training batch 573...\n",
            "Training batch 574...\n",
            "Training batch 575...\n",
            "Training batch 576...\n",
            "Training batch 577...\n",
            "Training batch 578...\n",
            "Training batch 579...\n",
            "Training batch 580...\n",
            "Training batch 581...\n",
            "Training batch 582...\n",
            "Training batch 583...\n",
            "Training batch 584...\n",
            "Training batch 585...\n",
            "Training batch 586...\n",
            "Training batch 587...\n",
            "Training batch 588...\n",
            "Training batch 589...\n",
            "Training batch 590...\n",
            "Training batch 591...\n",
            "Training batch 592...\n",
            "Training batch 593...\n",
            "Training batch 594...\n",
            "Training batch 595...\n",
            "Training batch 596...\n",
            "Training batch 597...\n",
            "Training batch 598...\n",
            "Training batch 599...\n",
            "Training batch 600...\n",
            "Training batch 601...\n",
            "Training batch 602...\n",
            "Training batch 603...\n",
            "Training batch 604...\n",
            "Training batch 605...\n",
            "Training batch 606...\n",
            "Training batch 607...\n",
            "Training batch 608...\n",
            "Training batch 609...\n",
            "Training batch 610...\n",
            "Training batch 611...\n",
            "Training batch 612...\n",
            "Training batch 613...\n",
            "Training batch 614...\n",
            "Training batch 615...\n",
            "Training batch 616...\n",
            "Training batch 617...\n",
            "Training batch 618...\n",
            "Training batch 619...\n",
            "Training batch 620...\n",
            "Training batch 621...\n",
            "Training batch 622...\n",
            "Training batch 623...\n",
            "Training batch 624...\n",
            "Training batch 625...\n",
            "Training batch 626...\n",
            "Training batch 627...\n",
            "Training batch 628...\n",
            "Training batch 629...\n",
            "Training batch 630...\n",
            "Training batch 631...\n",
            "Training batch 632...\n",
            "Training batch 633...\n",
            "Training batch 634...\n",
            "Training batch 635...\n",
            "Training batch 636...\n",
            "Training batch 637...\n",
            "Training batch 638...\n",
            "Training batch 639...\n",
            "Training batch 640...\n",
            "Training batch 641...\n",
            "Training batch 642...\n",
            "Training batch 643...\n",
            "Training batch 644...\n",
            "Training batch 645...\n",
            "Training batch 646...\n",
            "Training batch 647...\n",
            "Training batch 648...\n",
            "Training batch 649...\n",
            "Training batch 650...\n",
            "Training batch 651...\n",
            "Training batch 652...\n",
            "Training batch 653...\n",
            "Training batch 654...\n",
            "Training batch 655...\n",
            "Training batch 656...\n",
            "Training batch 657...\n",
            "Training batch 658...\n",
            "Training batch 659...\n",
            "Training batch 660...\n",
            "Training batch 661...\n",
            "Training batch 662...\n",
            "Training batch 663...\n",
            "Training batch 664...\n",
            "Training batch 665...\n",
            "Training batch 666...\n",
            "Training batch 667...\n",
            "Training batch 668...\n",
            "Training batch 669...\n",
            "Training batch 670...\n",
            "Training batch 671...\n",
            "Training batch 672...\n",
            "Training batch 673...\n",
            "Training batch 674...\n",
            "Training batch 675...\n",
            "Training batch 676...\n",
            "Training batch 677...\n",
            "Training batch 678...\n",
            "Training batch 679...\n",
            "Training batch 680...\n",
            "Training batch 681...\n",
            "Training batch 682...\n",
            "Training batch 683...\n",
            "Training batch 684...\n",
            "Training batch 685...\n",
            "Training batch 686...\n",
            "Training batch 687...\n",
            "Training batch 688...\n",
            "Training batch 689...\n",
            "Training batch 690...\n",
            "Training batch 691...\n",
            "Training batch 692...\n",
            "Training batch 693...\n",
            "Training batch 694...\n",
            "Training batch 695...\n",
            "Training batch 696...\n",
            "Training batch 697...\n",
            "Training batch 698...\n",
            "Training batch 699...\n",
            "Training batch 700...\n",
            "Training batch 701...\n",
            "Training batch 702...\n",
            "Training batch 703...\n",
            "Training batch 704...\n",
            "Training batch 705...\n",
            "Training batch 706...\n",
            "Training batch 707...\n",
            "Training batch 708...\n",
            "Training batch 709...\n",
            "Training batch 710...\n",
            "Training batch 711...\n",
            "Training batch 712...\n",
            "Training batch 713...\n",
            "Training batch 714...\n",
            "Training batch 715...\n",
            "Training batch 716...\n",
            "Training batch 717...\n",
            "Training batch 718...\n",
            "Training batch 719...\n",
            "Training batch 720...\n",
            "Training batch 721...\n",
            "Training batch 722...\n",
            "Training batch 723...\n",
            "Training batch 724...\n",
            "Training batch 725...\n",
            "Training batch 726...\n",
            "Training batch 727...\n",
            "Training batch 728...\n",
            "Training batch 729...\n",
            "Training batch 730...\n",
            "Training batch 731...\n",
            "Training batch 732...\n",
            "Training batch 733...\n",
            "Training batch 734...\n",
            "Training batch 735...\n",
            "Training batch 736...\n",
            "Training batch 737...\n",
            "Training batch 738...\n",
            "Training batch 739...\n",
            "Training batch 740...\n",
            "Training batch 741...\n",
            "Training batch 742...\n",
            "Training batch 743...\n",
            "Training batch 744...\n",
            "Training batch 745...\n",
            "Training batch 746...\n",
            "Training batch 747...\n",
            "Training batch 748...\n",
            "Training batch 749...\n",
            "Training batch 750...\n",
            "Training batch 751...\n",
            "Training batch 752...\n",
            "Training batch 753...\n",
            "Training batch 754...\n",
            "Training batch 755...\n",
            "Training batch 756...\n",
            "Training batch 757...\n",
            "Training batch 758...\n",
            "Training batch 759...\n",
            "Training batch 760...\n",
            "Training batch 761...\n",
            "Training batch 762...\n",
            "Training batch 763...\n",
            "Training batch 764...\n",
            "Training batch 765...\n",
            "Training batch 766...\n",
            "Training batch 767...\n",
            "Training batch 768...\n",
            "Training batch 769...\n",
            "Training batch 770...\n",
            "Training batch 771...\n",
            "Training batch 772...\n",
            "Training batch 773...\n",
            "Training batch 774...\n",
            "Training batch 775...\n",
            "Training batch 776...\n",
            "Training batch 777...\n",
            "Training batch 778...\n",
            "Training batch 779...\n",
            "Training batch 780...\n",
            "Training batch 781...\n",
            "Training batch 782...\n",
            "Training batch 783...\n",
            "Training batch 784...\n",
            "Training batch 785...\n",
            "Training batch 786...\n",
            "Training batch 787...\n",
            "Training batch 788...\n",
            "Training batch 789...\n",
            "Training batch 790...\n",
            "Training batch 791...\n",
            "Training batch 792...\n",
            "Training batch 793...\n",
            "Training batch 794...\n",
            "Training batch 795...\n",
            "Training batch 796...\n",
            "Training batch 797...\n",
            "Training batch 798...\n",
            "Training batch 799...\n",
            "Training batch 800...\n",
            "Training batch 801...\n",
            "Training batch 802...\n",
            "Training batch 803...\n",
            "Training batch 804...\n",
            "Training batch 805...\n",
            "Training batch 806...\n",
            "Training batch 807...\n",
            "Training batch 808...\n",
            "Training batch 809...\n",
            "Training batch 810...\n",
            "Training batch 811...\n",
            "Training batch 812...\n",
            "Training batch 813...\n",
            "Training batch 814...\n",
            "Training batch 815...\n",
            "Training batch 816...\n",
            "Training batch 817...\n",
            "Training batch 818...\n",
            "Training batch 819...\n",
            "Training batch 820...\n",
            "Training batch 821...\n",
            "Training batch 822...\n",
            "Training batch 823...\n",
            "Training batch 824...\n",
            "Training batch 825...\n",
            "Training batch 826...\n",
            "Training batch 827...\n",
            "Training batch 828...\n",
            "Training batch 829...\n",
            "Training batch 830...\n",
            "Training batch 831...\n",
            "Training batch 832...\n",
            "Training batch 833...\n",
            "Training batch 834...\n",
            "Training batch 835...\n",
            "Training batch 836...\n",
            "Training batch 837...\n",
            "Training batch 838...\n",
            "Training batch 839...\n",
            "Training batch 840...\n",
            "Training batch 841...\n",
            "Training batch 842...\n",
            "Training batch 843...\n",
            "Training batch 844...\n",
            "Training batch 845...\n",
            "Training batch 846...\n",
            "Training batch 847...\n",
            "Training batch 848...\n",
            "Training batch 849...\n",
            "Training batch 850...\n",
            "Training batch 851...\n",
            "Training batch 852...\n",
            "Training batch 853...\n",
            "Training batch 854...\n",
            "Training batch 855...\n",
            "Training batch 856...\n",
            "Training batch 857...\n",
            "Training batch 858...\n",
            "Training batch 859...\n",
            "Training batch 860...\n",
            "Training batch 861...\n",
            "Training batch 862...\n",
            "Training batch 863...\n",
            "Training batch 864...\n",
            "Training batch 865...\n",
            "Training batch 866...\n",
            "Training batch 867...\n",
            "Training batch 868...\n",
            "Training batch 869...\n",
            "Training batch 870...\n",
            "Training batch 871...\n",
            "Training batch 872...\n",
            "Training batch 873...\n",
            "Training batch 874...\n",
            "Training batch 875...\n",
            "Training batch 876...\n",
            "Training batch 877...\n",
            "Training batch 878...\n",
            "Training batch 879...\n",
            "Training batch 880...\n",
            "Training batch 881...\n",
            "Training batch 882...\n",
            "Training batch 883...\n",
            "Training batch 884...\n",
            "Training batch 885...\n",
            "Training batch 886...\n",
            "Training batch 887...\n",
            "Training batch 888...\n",
            "Training batch 889...\n",
            "Training batch 890...\n",
            "Training batch 891...\n",
            "Training batch 892...\n",
            "Training batch 893...\n",
            "Training batch 894...\n",
            "Training batch 895...\n",
            "Training batch 896...\n",
            "Training batch 897...\n",
            "Training batch 898...\n",
            "Training batch 899...\n",
            "Training batch 900...\n",
            "Training batch 901...\n",
            "Training batch 902...\n",
            "Training batch 903...\n",
            "Training batch 904...\n",
            "Training batch 905...\n",
            "Training batch 906...\n",
            "Training batch 907...\n",
            "Training batch 908...\n",
            "Training batch 909...\n",
            "Training batch 910...\n",
            "Training batch 911...\n",
            "Training batch 912...\n",
            "Training batch 913...\n",
            "Training batch 914...\n",
            "Training batch 915...\n",
            "Training batch 916...\n",
            "Training batch 917...\n",
            "Training batch 918...\n",
            "Training batch 919...\n",
            "Training batch 920...\n",
            "Training batch 921...\n",
            "Training batch 922...\n",
            "Training batch 923...\n",
            "Training batch 924...\n",
            "Training batch 925...\n",
            "Training batch 926...\n",
            "Training batch 927...\n",
            "Training batch 928...\n",
            "Training batch 929...\n",
            "Training batch 930...\n",
            "Training batch 931...\n",
            "Training batch 932...\n",
            "Training batch 933...\n",
            "Training batch 934...\n",
            "Training batch 935...\n",
            "Training batch 936...\n",
            "Training batch 937...\n",
            "Training batch 938...\n",
            "Training batch 939...\n",
            "Training batch 940...\n",
            "Training batch 941...\n",
            "Training batch 942...\n",
            "Training batch 943...\n",
            "Training batch 944...\n",
            "Training batch 945...\n",
            "Training batch 946...\n",
            "Training batch 947...\n",
            "Training batch 948...\n",
            "Training batch 949...\n",
            "Training batch 950...\n",
            "Training batch 951...\n",
            "Training batch 952...\n",
            "Training batch 953...\n",
            "Training batch 954...\n",
            "Training batch 955...\n",
            "Training batch 956...\n",
            "Training batch 957...\n",
            "Training batch 958...\n",
            "Training batch 959...\n",
            "Training batch 960...\n",
            "Training batch 961...\n",
            "Training batch 962...\n",
            "Training batch 963...\n",
            "Training batch 964...\n",
            "Training batch 965...\n",
            "Training batch 966...\n",
            "Training batch 967...\n",
            "Training batch 968...\n",
            "Training batch 969...\n",
            "Training batch 970...\n",
            "Training batch 971...\n",
            "Training batch 972...\n",
            "Training batch 973...\n",
            "Training batch 974...\n",
            "Training batch 975...\n",
            "Training batch 976...\n",
            "Training batch 977...\n",
            "Training batch 978...\n",
            "Training batch 979...\n",
            "Training batch 980...\n",
            "Training batch 981...\n",
            "Training batch 982...\n",
            "Training batch 983...\n",
            "Training batch 984...\n",
            "Training batch 985...\n",
            "Training batch 986...\n",
            "Training batch 987...\n",
            "Training batch 988...\n",
            "Training batch 989...\n",
            "Training batch 990...\n",
            "Training batch 991...\n",
            "Training batch 992...\n",
            "Training batch 993...\n",
            "Training batch 994...\n",
            "Training batch 995...\n",
            "Training batch 996...\n",
            "Training batch 997...\n",
            "Training batch 998...\n",
            "Training batch 999...\n",
            "Training batch 1000...\n",
            "Training batch 1001...\n",
            "Training batch 1002...\n",
            "Training batch 1003...\n",
            "Training batch 1004...\n",
            "Training batch 1005...\n",
            "Training batch 1006...\n",
            "Training batch 1007...\n",
            "Training batch 1008...\n",
            "Training batch 1009...\n",
            "Training batch 1010...\n",
            "Training batch 1011...\n",
            "Training batch 1012...\n",
            "Training batch 1013...\n",
            "Training batch 1014...\n",
            "Training batch 1015...\n",
            "Training batch 1016...\n",
            "Training batch 1017...\n",
            "Training batch 1018...\n",
            "Training batch 1019...\n",
            "Training batch 1020...\n",
            "Training batch 1021...\n",
            "Training batch 1022...\n",
            "Training batch 1023...\n",
            "Training batch 1024...\n",
            "Training batch 1025...\n",
            "Training batch 1026...\n",
            "Training batch 1027...\n",
            "Training batch 1028...\n",
            "Training batch 1029...\n",
            "Training batch 1030...\n",
            "Training batch 1031...\n",
            "Training batch 1032...\n",
            "Training batch 1033...\n",
            "Training batch 1034...\n",
            "Training batch 1035...\n",
            "Training batch 1036...\n",
            "Training batch 1037...\n",
            "Training batch 1038...\n",
            "Training batch 1039...\n",
            "Training batch 1040...\n",
            "Training batch 1041...\n",
            "Training batch 1042...\n",
            "Training batch 1043...\n",
            "Training batch 1044...\n",
            "Training batch 1045...\n",
            "Training batch 1046...\n",
            "Training batch 1047...\n",
            "Training batch 1048...\n",
            "Training batch 1049...\n",
            "Training batch 1050...\n",
            "Training batch 1051...\n",
            "Training batch 1052...\n",
            "Training batch 1053...\n",
            "Training batch 1054...\n",
            "Training batch 1055...\n",
            "Training batch 1056...\n",
            "Training batch 1057...\n",
            "Training batch 1058...\n",
            "Training batch 1059...\n",
            "Training batch 1060...\n",
            "Training batch 1061...\n",
            "Training batch 1062...\n",
            "Training batch 1063...\n",
            "Training batch 1064...\n",
            "Training batch 1065...\n",
            "Training batch 1066...\n",
            "Training batch 1067...\n",
            "Training batch 1068...\n",
            "Training batch 1069...\n",
            "Training batch 1070...\n",
            "Training batch 1071...\n",
            "Training batch 1072...\n",
            "Training batch 1073...\n",
            "Training batch 1074...\n",
            "Training batch 1075...\n",
            "Training batch 1076...\n",
            "Training batch 1077...\n",
            "Training batch 1078...\n",
            "Training batch 1079...\n",
            "Training batch 1080...\n",
            "Training batch 1081...\n",
            "Training batch 1082...\n",
            "Training batch 1083...\n",
            "Training batch 1084...\n",
            "Training batch 1085...\n",
            "Training batch 1086...\n",
            "Training batch 1087...\n",
            "Training batch 1088...\n",
            "Training batch 1089...\n",
            "Training batch 1090...\n",
            "Training batch 1091...\n",
            "Training batch 1092...\n",
            "Training batch 1093...\n",
            "Training batch 1094...\n",
            "Training batch 1095...\n",
            "Training batch 1096...\n",
            "Training batch 1097...\n",
            "Training batch 1098...\n",
            "Training batch 1099...\n",
            "Training batch 1100...\n",
            "Training batch 1101...\n",
            "Training batch 1102...\n",
            "Training batch 1103...\n",
            "Training batch 1104...\n",
            "Training batch 1105...\n",
            "Training batch 1106...\n",
            "Training batch 1107...\n",
            "Training batch 1108...\n",
            "Training batch 1109...\n",
            "Training batch 1110...\n",
            "Training batch 1111...\n",
            "Training batch 1112...\n",
            "Training batch 1113...\n",
            "Training batch 1114...\n",
            "Training batch 1115...\n",
            "Training batch 1116...\n",
            "Training batch 1117...\n",
            "Training batch 1118...\n",
            "Training batch 1119...\n",
            "Training batch 1120...\n",
            "Training batch 1121...\n",
            "Training batch 1122...\n",
            "Training batch 1123...\n",
            "Training batch 1124...\n",
            "Training batch 1125...\n",
            "Training batch 1126...\n",
            "Training batch 1127...\n",
            "Training batch 1128...\n",
            "Training batch 1129...\n",
            "Training batch 1130...\n",
            "Training batch 1131...\n",
            "Training batch 1132...\n",
            "Training batch 1133...\n",
            "Training batch 1134...\n",
            "Training batch 1135...\n",
            "Training batch 1136...\n",
            "Training batch 1137...\n",
            "Training batch 1138...\n",
            "Training batch 1139...\n",
            "Training batch 1140...\n",
            "Training batch 1141...\n",
            "Training batch 1142...\n",
            "Training batch 1143...\n",
            "Training batch 1144...\n",
            "Training batch 1145...\n",
            "Training batch 1146...\n",
            "Training batch 1147...\n",
            "Training batch 1148...\n",
            "Training batch 1149...\n",
            "Training batch 1150...\n",
            "Training batch 1151...\n",
            "Training batch 1152...\n",
            "Training batch 1153...\n",
            "Training batch 1154...\n",
            "Training batch 1155...\n",
            "Training batch 1156...\n",
            "Training batch 1157...\n",
            "Training batch 1158...\n",
            "Training batch 1159...\n",
            "Training batch 1160...\n",
            "Training batch 1161...\n",
            "Training batch 1162...\n",
            "Training batch 1163...\n",
            "Training batch 1164...\n",
            "Training batch 1165...\n",
            "Training batch 1166...\n",
            "Training batch 1167...\n",
            "Training batch 1168...\n",
            "Training batch 1169...\n",
            "Training batch 1170...\n",
            "Training batch 1171...\n",
            "Training batch 1172...\n",
            "Training batch 1173...\n",
            "Training batch 1174...\n",
            "Training batch 1175...\n",
            "Training batch 1176...\n",
            "Training batch 1177...\n",
            "Training batch 1178...\n",
            "Training batch 1179...\n",
            "Training batch 1180...\n",
            "Training batch 1181...\n",
            "Training batch 1182...\n",
            "Training batch 1183...\n",
            "Training batch 1184...\n",
            "Training batch 1185...\n",
            "Training batch 1186...\n",
            "Training batch 1187...\n",
            "Training batch 1188...\n",
            "Training batch 1189...\n",
            "Training batch 1190...\n",
            "Training batch 1191...\n",
            "Training batch 1192...\n",
            "Training batch 1193...\n",
            "Training batch 1194...\n",
            "Training batch 1195...\n",
            "Training batch 1196...\n",
            "Training batch 1197...\n",
            "Training batch 1198...\n",
            "Training batch 1199...\n",
            "Training batch 1200...\n",
            "Training batch 1201...\n",
            "Training batch 1202...\n",
            "Training batch 1203...\n",
            "Training batch 1204...\n",
            "Training batch 1205...\n",
            "Training batch 1206...\n",
            "Training batch 1207...\n",
            "Training batch 1208...\n",
            "Training batch 1209...\n",
            "Training batch 1210...\n",
            "Training batch 1211...\n",
            "Training batch 1212...\n",
            "Training batch 1213...\n",
            "Training batch 1214...\n",
            "Training batch 1215...\n",
            "Training batch 1216...\n",
            "Training batch 1217...\n",
            "Training batch 1218...\n",
            "Training batch 1219...\n",
            "Training batch 1220...\n",
            "Training batch 1221...\n",
            "Training batch 1222...\n",
            "Training batch 1223...\n",
            "Training batch 1224...\n",
            "Training batch 1225...\n",
            "Training batch 1226...\n",
            "Training batch 1227...\n",
            "Training batch 1228...\n",
            "Training batch 1229...\n",
            "Training batch 1230...\n",
            "Training batch 1231...\n",
            "Training batch 1232...\n",
            "Training batch 1233...\n",
            "Training batch 1234...\n",
            "Training batch 1235...\n",
            "Training batch 1236...\n",
            "Training batch 1237...\n",
            "Training batch 1238...\n",
            "Training batch 1239...\n",
            "Training batch 1240...\n",
            "Training batch 1241...\n",
            "Training batch 1242...\n",
            "Training batch 1243...\n",
            "Training batch 1244...\n",
            "Training batch 1245...\n",
            "Training batch 1246...\n",
            "Training batch 1247...\n",
            "Training batch 1248...\n",
            "Training batch 1249...\n",
            "Training batch 1250...\n",
            "Training batch 1251...\n",
            "Training batch 1252...\n",
            "Training batch 1253...\n",
            "Training batch 1254...\n",
            "Training batch 1255...\n",
            "Training batch 1256...\n",
            "Training batch 1257...\n",
            "Training batch 1258...\n",
            "Training batch 1259...\n",
            "Training batch 1260...\n",
            "Training batch 1261...\n",
            "Training batch 1262...\n",
            "Training batch 1263...\n",
            "Training batch 1264...\n",
            "Training batch 1265...\n",
            "Training batch 1266...\n",
            "Training batch 1267...\n",
            "Training batch 1268...\n",
            "Training batch 1269...\n",
            "Training batch 1270...\n",
            "Training batch 1271...\n",
            "Training batch 1272...\n",
            "Training batch 1273...\n",
            "Training batch 1274...\n",
            "Training batch 1275...\n",
            "Training batch 1276...\n",
            "Training batch 1277...\n",
            "Training batch 1278...\n",
            "Training batch 1279...\n",
            "Training batch 1280...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKFWSdX7P-zA"
      },
      "source": [
        "model_svm = SVC()\n",
        "model_svm.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3jFSltlhRQ7"
      },
      "source": [
        "def train_model_rf(aName, x,y):\n",
        "  model = aName\n",
        "  batch = 1\n",
        "  for i in range(0,x.shape[0],1000):\n",
        "    print(\"Training batch {}...\".format(batch))\n",
        "    batch += 1\n",
        "    model.fit(x[i:i+1000,:].todense().astype(np.float16), y[i:i+1000])\n",
        "    model.n_estimators +=1\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYNpYcN6jVS6",
        "outputId": "e9d3b28e-21d8-43de-9bc5-53168ea1fbdc"
      },
      "source": [
        "clf = RandomForestClassifier(warm_start = True, n_estimators = 1)\n",
        "model_rf = train_model_rf(clf,x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training batch 1...\n",
            "Training batch 2...\n",
            "Training batch 3...\n",
            "Training batch 4...\n",
            "Training batch 5...\n",
            "Training batch 6...\n",
            "Training batch 7...\n",
            "Training batch 8...\n",
            "Training batch 9...\n",
            "Training batch 10...\n",
            "Training batch 11...\n",
            "Training batch 12...\n",
            "Training batch 13...\n",
            "Training batch 14...\n",
            "Training batch 15...\n",
            "Training batch 16...\n",
            "Training batch 17...\n",
            "Training batch 18...\n",
            "Training batch 19...\n",
            "Training batch 20...\n",
            "Training batch 21...\n",
            "Training batch 22...\n",
            "Training batch 23...\n",
            "Training batch 24...\n",
            "Training batch 25...\n",
            "Training batch 26...\n",
            "Training batch 27...\n",
            "Training batch 28...\n",
            "Training batch 29...\n",
            "Training batch 30...\n",
            "Training batch 31...\n",
            "Training batch 32...\n",
            "Training batch 33...\n",
            "Training batch 34...\n",
            "Training batch 35...\n",
            "Training batch 36...\n",
            "Training batch 37...\n",
            "Training batch 38...\n",
            "Training batch 39...\n",
            "Training batch 40...\n",
            "Training batch 41...\n",
            "Training batch 42...\n",
            "Training batch 43...\n",
            "Training batch 44...\n",
            "Training batch 45...\n",
            "Training batch 46...\n",
            "Training batch 47...\n",
            "Training batch 48...\n",
            "Training batch 49...\n",
            "Training batch 50...\n",
            "Training batch 51...\n",
            "Training batch 52...\n",
            "Training batch 53...\n",
            "Training batch 54...\n",
            "Training batch 55...\n",
            "Training batch 56...\n",
            "Training batch 57...\n",
            "Training batch 58...\n",
            "Training batch 59...\n",
            "Training batch 60...\n",
            "Training batch 61...\n",
            "Training batch 62...\n",
            "Training batch 63...\n",
            "Training batch 64...\n",
            "Training batch 65...\n",
            "Training batch 66...\n",
            "Training batch 67...\n",
            "Training batch 68...\n",
            "Training batch 69...\n",
            "Training batch 70...\n",
            "Training batch 71...\n",
            "Training batch 72...\n",
            "Training batch 73...\n",
            "Training batch 74...\n",
            "Training batch 75...\n",
            "Training batch 76...\n",
            "Training batch 77...\n",
            "Training batch 78...\n",
            "Training batch 79...\n",
            "Training batch 80...\n",
            "Training batch 81...\n",
            "Training batch 82...\n",
            "Training batch 83...\n",
            "Training batch 84...\n",
            "Training batch 85...\n",
            "Training batch 86...\n",
            "Training batch 87...\n",
            "Training batch 88...\n",
            "Training batch 89...\n",
            "Training batch 90...\n",
            "Training batch 91...\n",
            "Training batch 92...\n",
            "Training batch 93...\n",
            "Training batch 94...\n",
            "Training batch 95...\n",
            "Training batch 96...\n",
            "Training batch 97...\n",
            "Training batch 98...\n",
            "Training batch 99...\n",
            "Training batch 100...\n",
            "Training batch 101...\n",
            "Training batch 102...\n",
            "Training batch 103...\n",
            "Training batch 104...\n",
            "Training batch 105...\n",
            "Training batch 106...\n",
            "Training batch 107...\n",
            "Training batch 108...\n",
            "Training batch 109...\n",
            "Training batch 110...\n",
            "Training batch 111...\n",
            "Training batch 112...\n",
            "Training batch 113...\n",
            "Training batch 114...\n",
            "Training batch 115...\n",
            "Training batch 116...\n",
            "Training batch 117...\n",
            "Training batch 118...\n",
            "Training batch 119...\n",
            "Training batch 120...\n",
            "Training batch 121...\n",
            "Training batch 122...\n",
            "Training batch 123...\n",
            "Training batch 124...\n",
            "Training batch 125...\n",
            "Training batch 126...\n",
            "Training batch 127...\n",
            "Training batch 128...\n",
            "Training batch 129...\n",
            "Training batch 130...\n",
            "Training batch 131...\n",
            "Training batch 132...\n",
            "Training batch 133...\n",
            "Training batch 134...\n",
            "Training batch 135...\n",
            "Training batch 136...\n",
            "Training batch 137...\n",
            "Training batch 138...\n",
            "Training batch 139...\n",
            "Training batch 140...\n",
            "Training batch 141...\n",
            "Training batch 142...\n",
            "Training batch 143...\n",
            "Training batch 144...\n",
            "Training batch 145...\n",
            "Training batch 146...\n",
            "Training batch 147...\n",
            "Training batch 148...\n",
            "Training batch 149...\n",
            "Training batch 150...\n",
            "Training batch 151...\n",
            "Training batch 152...\n",
            "Training batch 153...\n",
            "Training batch 154...\n",
            "Training batch 155...\n",
            "Training batch 156...\n",
            "Training batch 157...\n",
            "Training batch 158...\n",
            "Training batch 159...\n",
            "Training batch 160...\n",
            "Training batch 161...\n",
            "Training batch 162...\n",
            "Training batch 163...\n",
            "Training batch 164...\n",
            "Training batch 165...\n",
            "Training batch 166...\n",
            "Training batch 167...\n",
            "Training batch 168...\n",
            "Training batch 169...\n",
            "Training batch 170...\n",
            "Training batch 171...\n",
            "Training batch 172...\n",
            "Training batch 173...\n",
            "Training batch 174...\n",
            "Training batch 175...\n",
            "Training batch 176...\n",
            "Training batch 177...\n",
            "Training batch 178...\n",
            "Training batch 179...\n",
            "Training batch 180...\n",
            "Training batch 181...\n",
            "Training batch 182...\n",
            "Training batch 183...\n",
            "Training batch 184...\n",
            "Training batch 185...\n",
            "Training batch 186...\n",
            "Training batch 187...\n",
            "Training batch 188...\n",
            "Training batch 189...\n",
            "Training batch 190...\n",
            "Training batch 191...\n",
            "Training batch 192...\n",
            "Training batch 193...\n",
            "Training batch 194...\n",
            "Training batch 195...\n",
            "Training batch 196...\n",
            "Training batch 197...\n",
            "Training batch 198...\n",
            "Training batch 199...\n",
            "Training batch 200...\n",
            "Training batch 201...\n",
            "Training batch 202...\n",
            "Training batch 203...\n",
            "Training batch 204...\n",
            "Training batch 205...\n",
            "Training batch 206...\n",
            "Training batch 207...\n",
            "Training batch 208...\n",
            "Training batch 209...\n",
            "Training batch 210...\n",
            "Training batch 211...\n",
            "Training batch 212...\n",
            "Training batch 213...\n",
            "Training batch 214...\n",
            "Training batch 215...\n",
            "Training batch 216...\n",
            "Training batch 217...\n",
            "Training batch 218...\n",
            "Training batch 219...\n",
            "Training batch 220...\n",
            "Training batch 221...\n",
            "Training batch 222...\n",
            "Training batch 223...\n",
            "Training batch 224...\n",
            "Training batch 225...\n",
            "Training batch 226...\n",
            "Training batch 227...\n",
            "Training batch 228...\n",
            "Training batch 229...\n",
            "Training batch 230...\n",
            "Training batch 231...\n",
            "Training batch 232...\n",
            "Training batch 233...\n",
            "Training batch 234...\n",
            "Training batch 235...\n",
            "Training batch 236...\n",
            "Training batch 237...\n",
            "Training batch 238...\n",
            "Training batch 239...\n",
            "Training batch 240...\n",
            "Training batch 241...\n",
            "Training batch 242...\n",
            "Training batch 243...\n",
            "Training batch 244...\n",
            "Training batch 245...\n",
            "Training batch 246...\n",
            "Training batch 247...\n",
            "Training batch 248...\n",
            "Training batch 249...\n",
            "Training batch 250...\n",
            "Training batch 251...\n",
            "Training batch 252...\n",
            "Training batch 253...\n",
            "Training batch 254...\n",
            "Training batch 255...\n",
            "Training batch 256...\n",
            "Training batch 257...\n",
            "Training batch 258...\n",
            "Training batch 259...\n",
            "Training batch 260...\n",
            "Training batch 261...\n",
            "Training batch 262...\n",
            "Training batch 263...\n",
            "Training batch 264...\n",
            "Training batch 265...\n",
            "Training batch 266...\n",
            "Training batch 267...\n",
            "Training batch 268...\n",
            "Training batch 269...\n",
            "Training batch 270...\n",
            "Training batch 271...\n",
            "Training batch 272...\n",
            "Training batch 273...\n",
            "Training batch 274...\n",
            "Training batch 275...\n",
            "Training batch 276...\n",
            "Training batch 277...\n",
            "Training batch 278...\n",
            "Training batch 279...\n",
            "Training batch 280...\n",
            "Training batch 281...\n",
            "Training batch 282...\n",
            "Training batch 283...\n",
            "Training batch 284...\n",
            "Training batch 285...\n",
            "Training batch 286...\n",
            "Training batch 287...\n",
            "Training batch 288...\n",
            "Training batch 289...\n",
            "Training batch 290...\n",
            "Training batch 291...\n",
            "Training batch 292...\n",
            "Training batch 293...\n",
            "Training batch 294...\n",
            "Training batch 295...\n",
            "Training batch 296...\n",
            "Training batch 297...\n",
            "Training batch 298...\n",
            "Training batch 299...\n",
            "Training batch 300...\n",
            "Training batch 301...\n",
            "Training batch 302...\n",
            "Training batch 303...\n",
            "Training batch 304...\n",
            "Training batch 305...\n",
            "Training batch 306...\n",
            "Training batch 307...\n",
            "Training batch 308...\n",
            "Training batch 309...\n",
            "Training batch 310...\n",
            "Training batch 311...\n",
            "Training batch 312...\n",
            "Training batch 313...\n",
            "Training batch 314...\n",
            "Training batch 315...\n",
            "Training batch 316...\n",
            "Training batch 317...\n",
            "Training batch 318...\n",
            "Training batch 319...\n",
            "Training batch 320...\n",
            "Training batch 321...\n",
            "Training batch 322...\n",
            "Training batch 323...\n",
            "Training batch 324...\n",
            "Training batch 325...\n",
            "Training batch 326...\n",
            "Training batch 327...\n",
            "Training batch 328...\n",
            "Training batch 329...\n",
            "Training batch 330...\n",
            "Training batch 331...\n",
            "Training batch 332...\n",
            "Training batch 333...\n",
            "Training batch 334...\n",
            "Training batch 335...\n",
            "Training batch 336...\n",
            "Training batch 337...\n",
            "Training batch 338...\n",
            "Training batch 339...\n",
            "Training batch 340...\n",
            "Training batch 341...\n",
            "Training batch 342...\n",
            "Training batch 343...\n",
            "Training batch 344...\n",
            "Training batch 345...\n",
            "Training batch 346...\n",
            "Training batch 347...\n",
            "Training batch 348...\n",
            "Training batch 349...\n",
            "Training batch 350...\n",
            "Training batch 351...\n",
            "Training batch 352...\n",
            "Training batch 353...\n",
            "Training batch 354...\n",
            "Training batch 355...\n",
            "Training batch 356...\n",
            "Training batch 357...\n",
            "Training batch 358...\n",
            "Training batch 359...\n",
            "Training batch 360...\n",
            "Training batch 361...\n",
            "Training batch 362...\n",
            "Training batch 363...\n",
            "Training batch 364...\n",
            "Training batch 365...\n",
            "Training batch 366...\n",
            "Training batch 367...\n",
            "Training batch 368...\n",
            "Training batch 369...\n",
            "Training batch 370...\n",
            "Training batch 371...\n",
            "Training batch 372...\n",
            "Training batch 373...\n",
            "Training batch 374...\n",
            "Training batch 375...\n",
            "Training batch 376...\n",
            "Training batch 377...\n",
            "Training batch 378...\n",
            "Training batch 379...\n",
            "Training batch 380...\n",
            "Training batch 381...\n",
            "Training batch 382...\n",
            "Training batch 383...\n",
            "Training batch 384...\n",
            "Training batch 385...\n",
            "Training batch 386...\n",
            "Training batch 387...\n",
            "Training batch 388...\n",
            "Training batch 389...\n",
            "Training batch 390...\n",
            "Training batch 391...\n",
            "Training batch 392...\n",
            "Training batch 393...\n",
            "Training batch 394...\n",
            "Training batch 395...\n",
            "Training batch 396...\n",
            "Training batch 397...\n",
            "Training batch 398...\n",
            "Training batch 399...\n",
            "Training batch 400...\n",
            "Training batch 401...\n",
            "Training batch 402...\n",
            "Training batch 403...\n",
            "Training batch 404...\n",
            "Training batch 405...\n",
            "Training batch 406...\n",
            "Training batch 407...\n",
            "Training batch 408...\n",
            "Training batch 409...\n",
            "Training batch 410...\n",
            "Training batch 411...\n",
            "Training batch 412...\n",
            "Training batch 413...\n",
            "Training batch 414...\n",
            "Training batch 415...\n",
            "Training batch 416...\n",
            "Training batch 417...\n",
            "Training batch 418...\n",
            "Training batch 419...\n",
            "Training batch 420...\n",
            "Training batch 421...\n",
            "Training batch 422...\n",
            "Training batch 423...\n",
            "Training batch 424...\n",
            "Training batch 425...\n",
            "Training batch 426...\n",
            "Training batch 427...\n",
            "Training batch 428...\n",
            "Training batch 429...\n",
            "Training batch 430...\n",
            "Training batch 431...\n",
            "Training batch 432...\n",
            "Training batch 433...\n",
            "Training batch 434...\n",
            "Training batch 435...\n",
            "Training batch 436...\n",
            "Training batch 437...\n",
            "Training batch 438...\n",
            "Training batch 439...\n",
            "Training batch 440...\n",
            "Training batch 441...\n",
            "Training batch 442...\n",
            "Training batch 443...\n",
            "Training batch 444...\n",
            "Training batch 445...\n",
            "Training batch 446...\n",
            "Training batch 447...\n",
            "Training batch 448...\n",
            "Training batch 449...\n",
            "Training batch 450...\n",
            "Training batch 451...\n",
            "Training batch 452...\n",
            "Training batch 453...\n",
            "Training batch 454...\n",
            "Training batch 455...\n",
            "Training batch 456...\n",
            "Training batch 457...\n",
            "Training batch 458...\n",
            "Training batch 459...\n",
            "Training batch 460...\n",
            "Training batch 461...\n",
            "Training batch 462...\n",
            "Training batch 463...\n",
            "Training batch 464...\n",
            "Training batch 465...\n",
            "Training batch 466...\n",
            "Training batch 467...\n",
            "Training batch 468...\n",
            "Training batch 469...\n",
            "Training batch 470...\n",
            "Training batch 471...\n",
            "Training batch 472...\n",
            "Training batch 473...\n",
            "Training batch 474...\n",
            "Training batch 475...\n",
            "Training batch 476...\n",
            "Training batch 477...\n",
            "Training batch 478...\n",
            "Training batch 479...\n",
            "Training batch 480...\n",
            "Training batch 481...\n",
            "Training batch 482...\n",
            "Training batch 483...\n",
            "Training batch 484...\n",
            "Training batch 485...\n",
            "Training batch 486...\n",
            "Training batch 487...\n",
            "Training batch 488...\n",
            "Training batch 489...\n",
            "Training batch 490...\n",
            "Training batch 491...\n",
            "Training batch 492...\n",
            "Training batch 493...\n",
            "Training batch 494...\n",
            "Training batch 495...\n",
            "Training batch 496...\n",
            "Training batch 497...\n",
            "Training batch 498...\n",
            "Training batch 499...\n",
            "Training batch 500...\n",
            "Training batch 501...\n",
            "Training batch 502...\n",
            "Training batch 503...\n",
            "Training batch 504...\n",
            "Training batch 505...\n",
            "Training batch 506...\n",
            "Training batch 507...\n",
            "Training batch 508...\n",
            "Training batch 509...\n",
            "Training batch 510...\n",
            "Training batch 511...\n",
            "Training batch 512...\n",
            "Training batch 513...\n",
            "Training batch 514...\n",
            "Training batch 515...\n",
            "Training batch 516...\n",
            "Training batch 517...\n",
            "Training batch 518...\n",
            "Training batch 519...\n",
            "Training batch 520...\n",
            "Training batch 521...\n",
            "Training batch 522...\n",
            "Training batch 523...\n",
            "Training batch 524...\n",
            "Training batch 525...\n",
            "Training batch 526...\n",
            "Training batch 527...\n",
            "Training batch 528...\n",
            "Training batch 529...\n",
            "Training batch 530...\n",
            "Training batch 531...\n",
            "Training batch 532...\n",
            "Training batch 533...\n",
            "Training batch 534...\n",
            "Training batch 535...\n",
            "Training batch 536...\n",
            "Training batch 537...\n",
            "Training batch 538...\n",
            "Training batch 539...\n",
            "Training batch 540...\n",
            "Training batch 541...\n",
            "Training batch 542...\n",
            "Training batch 543...\n",
            "Training batch 544...\n",
            "Training batch 545...\n",
            "Training batch 546...\n",
            "Training batch 547...\n",
            "Training batch 548...\n",
            "Training batch 549...\n",
            "Training batch 550...\n",
            "Training batch 551...\n",
            "Training batch 552...\n",
            "Training batch 553...\n",
            "Training batch 554...\n",
            "Training batch 555...\n",
            "Training batch 556...\n",
            "Training batch 557...\n",
            "Training batch 558...\n",
            "Training batch 559...\n",
            "Training batch 560...\n",
            "Training batch 561...\n",
            "Training batch 562...\n",
            "Training batch 563...\n",
            "Training batch 564...\n",
            "Training batch 565...\n",
            "Training batch 566...\n",
            "Training batch 567...\n",
            "Training batch 568...\n",
            "Training batch 569...\n",
            "Training batch 570...\n",
            "Training batch 571...\n",
            "Training batch 572...\n",
            "Training batch 573...\n",
            "Training batch 574...\n",
            "Training batch 575...\n",
            "Training batch 576...\n",
            "Training batch 577...\n",
            "Training batch 578...\n",
            "Training batch 579...\n",
            "Training batch 580...\n",
            "Training batch 581...\n",
            "Training batch 582...\n",
            "Training batch 583...\n",
            "Training batch 584...\n",
            "Training batch 585...\n",
            "Training batch 586...\n",
            "Training batch 587...\n",
            "Training batch 588...\n",
            "Training batch 589...\n",
            "Training batch 590...\n",
            "Training batch 591...\n",
            "Training batch 592...\n",
            "Training batch 593...\n",
            "Training batch 594...\n",
            "Training batch 595...\n",
            "Training batch 596...\n",
            "Training batch 597...\n",
            "Training batch 598...\n",
            "Training batch 599...\n",
            "Training batch 600...\n",
            "Training batch 601...\n",
            "Training batch 602...\n",
            "Training batch 603...\n",
            "Training batch 604...\n",
            "Training batch 605...\n",
            "Training batch 606...\n",
            "Training batch 607...\n",
            "Training batch 608...\n",
            "Training batch 609...\n",
            "Training batch 610...\n",
            "Training batch 611...\n",
            "Training batch 612...\n",
            "Training batch 613...\n",
            "Training batch 614...\n",
            "Training batch 615...\n",
            "Training batch 616...\n",
            "Training batch 617...\n",
            "Training batch 618...\n",
            "Training batch 619...\n",
            "Training batch 620...\n",
            "Training batch 621...\n",
            "Training batch 622...\n",
            "Training batch 623...\n",
            "Training batch 624...\n",
            "Training batch 625...\n",
            "Training batch 626...\n",
            "Training batch 627...\n",
            "Training batch 628...\n",
            "Training batch 629...\n",
            "Training batch 630...\n",
            "Training batch 631...\n",
            "Training batch 632...\n",
            "Training batch 633...\n",
            "Training batch 634...\n",
            "Training batch 635...\n",
            "Training batch 636...\n",
            "Training batch 637...\n",
            "Training batch 638...\n",
            "Training batch 639...\n",
            "Training batch 640...\n",
            "Training batch 641...\n",
            "Training batch 642...\n",
            "Training batch 643...\n",
            "Training batch 644...\n",
            "Training batch 645...\n",
            "Training batch 646...\n",
            "Training batch 647...\n",
            "Training batch 648...\n",
            "Training batch 649...\n",
            "Training batch 650...\n",
            "Training batch 651...\n",
            "Training batch 652...\n",
            "Training batch 653...\n",
            "Training batch 654...\n",
            "Training batch 655...\n",
            "Training batch 656...\n",
            "Training batch 657...\n",
            "Training batch 658...\n",
            "Training batch 659...\n",
            "Training batch 660...\n",
            "Training batch 661...\n",
            "Training batch 662...\n",
            "Training batch 663...\n",
            "Training batch 664...\n",
            "Training batch 665...\n",
            "Training batch 666...\n",
            "Training batch 667...\n",
            "Training batch 668...\n",
            "Training batch 669...\n",
            "Training batch 670...\n",
            "Training batch 671...\n",
            "Training batch 672...\n",
            "Training batch 673...\n",
            "Training batch 674...\n",
            "Training batch 675...\n",
            "Training batch 676...\n",
            "Training batch 677...\n",
            "Training batch 678...\n",
            "Training batch 679...\n",
            "Training batch 680...\n",
            "Training batch 681...\n",
            "Training batch 682...\n",
            "Training batch 683...\n",
            "Training batch 684...\n",
            "Training batch 685...\n",
            "Training batch 686...\n",
            "Training batch 687...\n",
            "Training batch 688...\n",
            "Training batch 689...\n",
            "Training batch 690...\n",
            "Training batch 691...\n",
            "Training batch 692...\n",
            "Training batch 693...\n",
            "Training batch 694...\n",
            "Training batch 695...\n",
            "Training batch 696...\n",
            "Training batch 697...\n",
            "Training batch 698...\n",
            "Training batch 699...\n",
            "Training batch 700...\n",
            "Training batch 701...\n",
            "Training batch 702...\n",
            "Training batch 703...\n",
            "Training batch 704...\n",
            "Training batch 705...\n",
            "Training batch 706...\n",
            "Training batch 707...\n",
            "Training batch 708...\n",
            "Training batch 709...\n",
            "Training batch 710...\n",
            "Training batch 711...\n",
            "Training batch 712...\n",
            "Training batch 713...\n",
            "Training batch 714...\n",
            "Training batch 715...\n",
            "Training batch 716...\n",
            "Training batch 717...\n",
            "Training batch 718...\n",
            "Training batch 719...\n",
            "Training batch 720...\n",
            "Training batch 721...\n",
            "Training batch 722...\n",
            "Training batch 723...\n",
            "Training batch 724...\n",
            "Training batch 725...\n",
            "Training batch 726...\n",
            "Training batch 727...\n",
            "Training batch 728...\n",
            "Training batch 729...\n",
            "Training batch 730...\n",
            "Training batch 731...\n",
            "Training batch 732...\n",
            "Training batch 733...\n",
            "Training batch 734...\n",
            "Training batch 735...\n",
            "Training batch 736...\n",
            "Training batch 737...\n",
            "Training batch 738...\n",
            "Training batch 739...\n",
            "Training batch 740...\n",
            "Training batch 741...\n",
            "Training batch 742...\n",
            "Training batch 743...\n",
            "Training batch 744...\n",
            "Training batch 745...\n",
            "Training batch 746...\n",
            "Training batch 747...\n",
            "Training batch 748...\n",
            "Training batch 749...\n",
            "Training batch 750...\n",
            "Training batch 751...\n",
            "Training batch 752...\n",
            "Training batch 753...\n",
            "Training batch 754...\n",
            "Training batch 755...\n",
            "Training batch 756...\n",
            "Training batch 757...\n",
            "Training batch 758...\n",
            "Training batch 759...\n",
            "Training batch 760...\n",
            "Training batch 761...\n",
            "Training batch 762...\n",
            "Training batch 763...\n",
            "Training batch 764...\n",
            "Training batch 765...\n",
            "Training batch 766...\n",
            "Training batch 767...\n",
            "Training batch 768...\n",
            "Training batch 769...\n",
            "Training batch 770...\n",
            "Training batch 771...\n",
            "Training batch 772...\n",
            "Training batch 773...\n",
            "Training batch 774...\n",
            "Training batch 775...\n",
            "Training batch 776...\n",
            "Training batch 777...\n",
            "Training batch 778...\n",
            "Training batch 779...\n",
            "Training batch 780...\n",
            "Training batch 781...\n",
            "Training batch 782...\n",
            "Training batch 783...\n",
            "Training batch 784...\n",
            "Training batch 785...\n",
            "Training batch 786...\n",
            "Training batch 787...\n",
            "Training batch 788...\n",
            "Training batch 789...\n",
            "Training batch 790...\n",
            "Training batch 791...\n",
            "Training batch 792...\n",
            "Training batch 793...\n",
            "Training batch 794...\n",
            "Training batch 795...\n",
            "Training batch 796...\n",
            "Training batch 797...\n",
            "Training batch 798...\n",
            "Training batch 799...\n",
            "Training batch 800...\n",
            "Training batch 801...\n",
            "Training batch 802...\n",
            "Training batch 803...\n",
            "Training batch 804...\n",
            "Training batch 805...\n",
            "Training batch 806...\n",
            "Training batch 807...\n",
            "Training batch 808...\n",
            "Training batch 809...\n",
            "Training batch 810...\n",
            "Training batch 811...\n",
            "Training batch 812...\n",
            "Training batch 813...\n",
            "Training batch 814...\n",
            "Training batch 815...\n",
            "Training batch 816...\n",
            "Training batch 817...\n",
            "Training batch 818...\n",
            "Training batch 819...\n",
            "Training batch 820...\n",
            "Training batch 821...\n",
            "Training batch 822...\n",
            "Training batch 823...\n",
            "Training batch 824...\n",
            "Training batch 825...\n",
            "Training batch 826...\n",
            "Training batch 827...\n",
            "Training batch 828...\n",
            "Training batch 829...\n",
            "Training batch 830...\n",
            "Training batch 831...\n",
            "Training batch 832...\n",
            "Training batch 833...\n",
            "Training batch 834...\n",
            "Training batch 835...\n",
            "Training batch 836...\n",
            "Training batch 837...\n",
            "Training batch 838...\n",
            "Training batch 839...\n",
            "Training batch 840...\n",
            "Training batch 841...\n",
            "Training batch 842...\n",
            "Training batch 843...\n",
            "Training batch 844...\n",
            "Training batch 845...\n",
            "Training batch 846...\n",
            "Training batch 847...\n",
            "Training batch 848...\n",
            "Training batch 849...\n",
            "Training batch 850...\n",
            "Training batch 851...\n",
            "Training batch 852...\n",
            "Training batch 853...\n",
            "Training batch 854...\n",
            "Training batch 855...\n",
            "Training batch 856...\n",
            "Training batch 857...\n",
            "Training batch 858...\n",
            "Training batch 859...\n",
            "Training batch 860...\n",
            "Training batch 861...\n",
            "Training batch 862...\n",
            "Training batch 863...\n",
            "Training batch 864...\n",
            "Training batch 865...\n",
            "Training batch 866...\n",
            "Training batch 867...\n",
            "Training batch 868...\n",
            "Training batch 869...\n",
            "Training batch 870...\n",
            "Training batch 871...\n",
            "Training batch 872...\n",
            "Training batch 873...\n",
            "Training batch 874...\n",
            "Training batch 875...\n",
            "Training batch 876...\n",
            "Training batch 877...\n",
            "Training batch 878...\n",
            "Training batch 879...\n",
            "Training batch 880...\n",
            "Training batch 881...\n",
            "Training batch 882...\n",
            "Training batch 883...\n",
            "Training batch 884...\n",
            "Training batch 885...\n",
            "Training batch 886...\n",
            "Training batch 887...\n",
            "Training batch 888...\n",
            "Training batch 889...\n",
            "Training batch 890...\n",
            "Training batch 891...\n",
            "Training batch 892...\n",
            "Training batch 893...\n",
            "Training batch 894...\n",
            "Training batch 895...\n",
            "Training batch 896...\n",
            "Training batch 897...\n",
            "Training batch 898...\n",
            "Training batch 899...\n",
            "Training batch 900...\n",
            "Training batch 901...\n",
            "Training batch 902...\n",
            "Training batch 903...\n",
            "Training batch 904...\n",
            "Training batch 905...\n",
            "Training batch 906...\n",
            "Training batch 907...\n",
            "Training batch 908...\n",
            "Training batch 909...\n",
            "Training batch 910...\n",
            "Training batch 911...\n",
            "Training batch 912...\n",
            "Training batch 913...\n",
            "Training batch 914...\n",
            "Training batch 915...\n",
            "Training batch 916...\n",
            "Training batch 917...\n",
            "Training batch 918...\n",
            "Training batch 919...\n",
            "Training batch 920...\n",
            "Training batch 921...\n",
            "Training batch 922...\n",
            "Training batch 923...\n",
            "Training batch 924...\n",
            "Training batch 925...\n",
            "Training batch 926...\n",
            "Training batch 927...\n",
            "Training batch 928...\n",
            "Training batch 929...\n",
            "Training batch 930...\n",
            "Training batch 931...\n",
            "Training batch 932...\n",
            "Training batch 933...\n",
            "Training batch 934...\n",
            "Training batch 935...\n",
            "Training batch 936...\n",
            "Training batch 937...\n",
            "Training batch 938...\n",
            "Training batch 939...\n",
            "Training batch 940...\n",
            "Training batch 941...\n",
            "Training batch 942...\n",
            "Training batch 943...\n",
            "Training batch 944...\n",
            "Training batch 945...\n",
            "Training batch 946...\n",
            "Training batch 947...\n",
            "Training batch 948...\n",
            "Training batch 949...\n",
            "Training batch 950...\n",
            "Training batch 951...\n",
            "Training batch 952...\n",
            "Training batch 953...\n",
            "Training batch 954...\n",
            "Training batch 955...\n",
            "Training batch 956...\n",
            "Training batch 957...\n",
            "Training batch 958...\n",
            "Training batch 959...\n",
            "Training batch 960...\n",
            "Training batch 961...\n",
            "Training batch 962...\n",
            "Training batch 963...\n",
            "Training batch 964...\n",
            "Training batch 965...\n",
            "Training batch 966...\n",
            "Training batch 967...\n",
            "Training batch 968...\n",
            "Training batch 969...\n",
            "Training batch 970...\n",
            "Training batch 971...\n",
            "Training batch 972...\n",
            "Training batch 973...\n",
            "Training batch 974...\n",
            "Training batch 975...\n",
            "Training batch 976...\n",
            "Training batch 977...\n",
            "Training batch 978...\n",
            "Training batch 979...\n",
            "Training batch 980...\n",
            "Training batch 981...\n",
            "Training batch 982...\n",
            "Training batch 983...\n",
            "Training batch 984...\n",
            "Training batch 985...\n",
            "Training batch 986...\n",
            "Training batch 987...\n",
            "Training batch 988...\n",
            "Training batch 989...\n",
            "Training batch 990...\n",
            "Training batch 991...\n",
            "Training batch 992...\n",
            "Training batch 993...\n",
            "Training batch 994...\n",
            "Training batch 995...\n",
            "Training batch 996...\n",
            "Training batch 997...\n",
            "Training batch 998...\n",
            "Training batch 999...\n",
            "Training batch 1000...\n",
            "Training batch 1001...\n",
            "Training batch 1002...\n",
            "Training batch 1003...\n",
            "Training batch 1004...\n",
            "Training batch 1005...\n",
            "Training batch 1006...\n",
            "Training batch 1007...\n",
            "Training batch 1008...\n",
            "Training batch 1009...\n",
            "Training batch 1010...\n",
            "Training batch 1011...\n",
            "Training batch 1012...\n",
            "Training batch 1013...\n",
            "Training batch 1014...\n",
            "Training batch 1015...\n",
            "Training batch 1016...\n",
            "Training batch 1017...\n",
            "Training batch 1018...\n",
            "Training batch 1019...\n",
            "Training batch 1020...\n",
            "Training batch 1021...\n",
            "Training batch 1022...\n",
            "Training batch 1023...\n",
            "Training batch 1024...\n",
            "Training batch 1025...\n",
            "Training batch 1026...\n",
            "Training batch 1027...\n",
            "Training batch 1028...\n",
            "Training batch 1029...\n",
            "Training batch 1030...\n",
            "Training batch 1031...\n",
            "Training batch 1032...\n",
            "Training batch 1033...\n",
            "Training batch 1034...\n",
            "Training batch 1035...\n",
            "Training batch 1036...\n",
            "Training batch 1037...\n",
            "Training batch 1038...\n",
            "Training batch 1039...\n",
            "Training batch 1040...\n",
            "Training batch 1041...\n",
            "Training batch 1042...\n",
            "Training batch 1043...\n",
            "Training batch 1044...\n",
            "Training batch 1045...\n",
            "Training batch 1046...\n",
            "Training batch 1047...\n",
            "Training batch 1048...\n",
            "Training batch 1049...\n",
            "Training batch 1050...\n",
            "Training batch 1051...\n",
            "Training batch 1052...\n",
            "Training batch 1053...\n",
            "Training batch 1054...\n",
            "Training batch 1055...\n",
            "Training batch 1056...\n",
            "Training batch 1057...\n",
            "Training batch 1058...\n",
            "Training batch 1059...\n",
            "Training batch 1060...\n",
            "Training batch 1061...\n",
            "Training batch 1062...\n",
            "Training batch 1063...\n",
            "Training batch 1064...\n",
            "Training batch 1065...\n",
            "Training batch 1066...\n",
            "Training batch 1067...\n",
            "Training batch 1068...\n",
            "Training batch 1069...\n",
            "Training batch 1070...\n",
            "Training batch 1071...\n",
            "Training batch 1072...\n",
            "Training batch 1073...\n",
            "Training batch 1074...\n",
            "Training batch 1075...\n",
            "Training batch 1076...\n",
            "Training batch 1077...\n",
            "Training batch 1078...\n",
            "Training batch 1079...\n",
            "Training batch 1080...\n",
            "Training batch 1081...\n",
            "Training batch 1082...\n",
            "Training batch 1083...\n",
            "Training batch 1084...\n",
            "Training batch 1085...\n",
            "Training batch 1086...\n",
            "Training batch 1087...\n",
            "Training batch 1088...\n",
            "Training batch 1089...\n",
            "Training batch 1090...\n",
            "Training batch 1091...\n",
            "Training batch 1092...\n",
            "Training batch 1093...\n",
            "Training batch 1094...\n",
            "Training batch 1095...\n",
            "Training batch 1096...\n",
            "Training batch 1097...\n",
            "Training batch 1098...\n",
            "Training batch 1099...\n",
            "Training batch 1100...\n",
            "Training batch 1101...\n",
            "Training batch 1102...\n",
            "Training batch 1103...\n",
            "Training batch 1104...\n",
            "Training batch 1105...\n",
            "Training batch 1106...\n",
            "Training batch 1107...\n",
            "Training batch 1108...\n",
            "Training batch 1109...\n",
            "Training batch 1110...\n",
            "Training batch 1111...\n",
            "Training batch 1112...\n",
            "Training batch 1113...\n",
            "Training batch 1114...\n",
            "Training batch 1115...\n",
            "Training batch 1116...\n",
            "Training batch 1117...\n",
            "Training batch 1118...\n",
            "Training batch 1119...\n",
            "Training batch 1120...\n",
            "Training batch 1121...\n",
            "Training batch 1122...\n",
            "Training batch 1123...\n",
            "Training batch 1124...\n",
            "Training batch 1125...\n",
            "Training batch 1126...\n",
            "Training batch 1127...\n",
            "Training batch 1128...\n",
            "Training batch 1129...\n",
            "Training batch 1130...\n",
            "Training batch 1131...\n",
            "Training batch 1132...\n",
            "Training batch 1133...\n",
            "Training batch 1134...\n",
            "Training batch 1135...\n",
            "Training batch 1136...\n",
            "Training batch 1137...\n",
            "Training batch 1138...\n",
            "Training batch 1139...\n",
            "Training batch 1140...\n",
            "Training batch 1141...\n",
            "Training batch 1142...\n",
            "Training batch 1143...\n",
            "Training batch 1144...\n",
            "Training batch 1145...\n",
            "Training batch 1146...\n",
            "Training batch 1147...\n",
            "Training batch 1148...\n",
            "Training batch 1149...\n",
            "Training batch 1150...\n",
            "Training batch 1151...\n",
            "Training batch 1152...\n",
            "Training batch 1153...\n",
            "Training batch 1154...\n",
            "Training batch 1155...\n",
            "Training batch 1156...\n",
            "Training batch 1157...\n",
            "Training batch 1158...\n",
            "Training batch 1159...\n",
            "Training batch 1160...\n",
            "Training batch 1161...\n",
            "Training batch 1162...\n",
            "Training batch 1163...\n",
            "Training batch 1164...\n",
            "Training batch 1165...\n",
            "Training batch 1166...\n",
            "Training batch 1167...\n",
            "Training batch 1168...\n",
            "Training batch 1169...\n",
            "Training batch 1170...\n",
            "Training batch 1171...\n",
            "Training batch 1172...\n",
            "Training batch 1173...\n",
            "Training batch 1174...\n",
            "Training batch 1175...\n",
            "Training batch 1176...\n",
            "Training batch 1177...\n",
            "Training batch 1178...\n",
            "Training batch 1179...\n",
            "Training batch 1180...\n",
            "Training batch 1181...\n",
            "Training batch 1182...\n",
            "Training batch 1183...\n",
            "Training batch 1184...\n",
            "Training batch 1185...\n",
            "Training batch 1186...\n",
            "Training batch 1187...\n",
            "Training batch 1188...\n",
            "Training batch 1189...\n",
            "Training batch 1190...\n",
            "Training batch 1191...\n",
            "Training batch 1192...\n",
            "Training batch 1193...\n",
            "Training batch 1194...\n",
            "Training batch 1195...\n",
            "Training batch 1196...\n",
            "Training batch 1197...\n",
            "Training batch 1198...\n",
            "Training batch 1199...\n",
            "Training batch 1200...\n",
            "Training batch 1201...\n",
            "Training batch 1202...\n",
            "Training batch 1203...\n",
            "Training batch 1204...\n",
            "Training batch 1205...\n",
            "Training batch 1206...\n",
            "Training batch 1207...\n",
            "Training batch 1208...\n",
            "Training batch 1209...\n",
            "Training batch 1210...\n",
            "Training batch 1211...\n",
            "Training batch 1212...\n",
            "Training batch 1213...\n",
            "Training batch 1214...\n",
            "Training batch 1215...\n",
            "Training batch 1216...\n",
            "Training batch 1217...\n",
            "Training batch 1218...\n",
            "Training batch 1219...\n",
            "Training batch 1220...\n",
            "Training batch 1221...\n",
            "Training batch 1222...\n",
            "Training batch 1223...\n",
            "Training batch 1224...\n",
            "Training batch 1225...\n",
            "Training batch 1226...\n",
            "Training batch 1227...\n",
            "Training batch 1228...\n",
            "Training batch 1229...\n",
            "Training batch 1230...\n",
            "Training batch 1231...\n",
            "Training batch 1232...\n",
            "Training batch 1233...\n",
            "Training batch 1234...\n",
            "Training batch 1235...\n",
            "Training batch 1236...\n",
            "Training batch 1237...\n",
            "Training batch 1238...\n",
            "Training batch 1239...\n",
            "Training batch 1240...\n",
            "Training batch 1241...\n",
            "Training batch 1242...\n",
            "Training batch 1243...\n",
            "Training batch 1244...\n",
            "Training batch 1245...\n",
            "Training batch 1246...\n",
            "Training batch 1247...\n",
            "Training batch 1248...\n",
            "Training batch 1249...\n",
            "Training batch 1250...\n",
            "Training batch 1251...\n",
            "Training batch 1252...\n",
            "Training batch 1253...\n",
            "Training batch 1254...\n",
            "Training batch 1255...\n",
            "Training batch 1256...\n",
            "Training batch 1257...\n",
            "Training batch 1258...\n",
            "Training batch 1259...\n",
            "Training batch 1260...\n",
            "Training batch 1261...\n",
            "Training batch 1262...\n",
            "Training batch 1263...\n",
            "Training batch 1264...\n",
            "Training batch 1265...\n",
            "Training batch 1266...\n",
            "Training batch 1267...\n",
            "Training batch 1268...\n",
            "Training batch 1269...\n",
            "Training batch 1270...\n",
            "Training batch 1271...\n",
            "Training batch 1272...\n",
            "Training batch 1273...\n",
            "Training batch 1274...\n",
            "Training batch 1275...\n",
            "Training batch 1276...\n",
            "Training batch 1277...\n",
            "Training batch 1278...\n",
            "Training batch 1279...\n",
            "Training batch 1280...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNVw9SzhQoBk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7rjTAHvXwlF"
      },
      "source": [
        "# **4**. \n",
        "Compare all models together in terms of Precision, Recall and F1 score. Put all of\n",
        "these numbers in a nicely formatted dataframe. Answer the following questions: Which\n",
        "model performs the best? Why do you think this is? What do you think you can do to\n",
        "improve performance? (30 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cISy9xseUPNg"
      },
      "source": [
        "import sklearn.metrics as sm\n",
        "def evaluate_model(pred,y):\n",
        "  acc = sm.accuracy_score(y,pred)\n",
        "  pr = sm.precision_score(y,pred,pos_label=4)\n",
        "  re = sm.recall_score(y,pred,pos_label=4)\n",
        "  f1 = sm.f1_score(pred, y, average='macro')\n",
        "  return acc,pr,re,f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp4GcVDRQpb7"
      },
      "source": [
        "predict_nb = model_nb.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-7-I8ItV70l",
        "outputId": "06e490cc-ec92-4a95-fa6e-ef23ccb03637"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4, 4, ..., 0, 0, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUiDd8InUzm1",
        "outputId": "3aa12f27-de8f-4078-b643-81a532568db9"
      },
      "source": [
        "acc_nb,pr_nb,re_nb,f1_nb=evaluate_model(predict_nb,y_test)\n",
        "print(\"Accuracy : \"+str(acc_nb))\n",
        "print(\"Precision : \"+str(pr_nb))\n",
        "print(\"Recall : \"+str(re_nb))\n",
        "print(\"F1 Score : \"+str(f1_nb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.773221875\n",
            "Precision : 0.8003435127614991\n",
            "Recall : 0.7280783990200123\n",
            "F1 Score : 0.7727589595936349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDiZSfH4PQnx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQyMMHsdNsqt"
      },
      "source": [
        "predict_rf = model_rf.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOALKH3vN1-o",
        "outputId": "36ed3773-a5f6-4f52-9560-a490e607f9b5"
      },
      "source": [
        "acc_rf,pr_rf,re_rf,f1_rf=evaluate_model(predict_rf,y_test)\n",
        "print(\"Accuracy : \"+str(acc_rf))\n",
        "print(\"Precision : \"+str(pr_rf))\n",
        "print(\"Recall : \"+str(re_rf))\n",
        "print(\"F1 Score : \"+str(f1_rf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.73945\n",
            "Precision : 0.7265031037540645\n",
            "Recall : 0.7680403994950064\n",
            "F1 Score : 0.7392367060560856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bPj1atjar45"
      },
      "source": [
        "# **Please check the dataframe for metrics that is created at the end of question 3 cell right below this.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AevDOF3iRqqP"
      },
      "source": [
        "# **3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5MBv_3SRs53"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Aidjk46Lqm",
        "outputId": "bb700e24-b688-4ab5-ea16-aae89372c2b3"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL4Y28Coujrr"
      },
      "source": [
        "train_data = pd.read_csv(\"/content/sample_data/train.csv\",encoding = \"ISO-8859-1\")\n",
        "# test_data = pd.read_csv(\"/content/sample_data/test.csv\",encoding = \"ISO-8859-1\")\n",
        "seed_value = 2361\n",
        "train = train_data #.sample(n=100000).reset_index()\n",
        "train.loc[train.polarity == 4, \"polarity\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImBssmbGGYRJ",
        "outputId": "1f1a02fb-11cf-41f1-db51-4e571148a57a"
      },
      "source": [
        "train['polarity'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800000\n",
              "0    800000\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "zUwqVu_sEVwm",
        "outputId": "2a8ac0e5-ed01-40d2-f8b5-6d3109e0a26f"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1         0  ...  is upset that he can't update his Facebook by ...\n",
              "2         0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3         0  ...    my whole body feels itchy and like its on fire \n",
              "4         0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFyeVRsBvMtp"
      },
      "source": [
        "a_train,a_test = train_test_split(train[(train['polarity']==0) | (train['polarity']==1)], test_size=0.20, random_state=seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmEcpbxj7mBH"
      },
      "source": [
        "Before padding the reviews to equal length we need to tokenize the reviews into list of words. Then, we will use the Tokenizer class from the keras.preprocessing.text module to create a word-to-index dictionary. This step is essential as the embedding layer in keras requires the input data to be in an integer encoded format where each word is represented by a unique integer. https://keras.io/api/preprocessing/text/#tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "r2hEfQWjwbD_",
        "outputId": "41324344-f257-40bf-d615-0878eb1f43b5"
      },
      "source": [
        "a_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1474686</th>\n",
              "      <td>1</td>\n",
              "      <td>2065768709</td>\n",
              "      <td>Sun Jun 07 09:15:09 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bowchickawoowoo</td>\n",
              "      <td>@scarlettstvitus It helps me to put a bag of i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>867725</th>\n",
              "      <td>1</td>\n",
              "      <td>1677825607</td>\n",
              "      <td>Sat May 02 04:11:27 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>smiling_nasti</td>\n",
              "      <td>@tommcfly it's actually law of humans' life......</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191053</th>\n",
              "      <td>0</td>\n",
              "      <td>1969494477</td>\n",
              "      <td>Sat May 30 00:12:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tcjohnson</td>\n",
              "      <td>@ryanseacrest Son of a crap!  I thought Adam L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266955</th>\n",
              "      <td>0</td>\n",
              "      <td>1989094495</td>\n",
              "      <td>Mon Jun 01 00:19:40 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mariahvondita</td>\n",
              "      <td>Alright folks I think I'm going to go try and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1472945</th>\n",
              "      <td>1</td>\n",
              "      <td>2065462631</td>\n",
              "      <td>Sun Jun 07 08:38:46 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>kulit</td>\n",
              "      <td>loves that there's so much Federer love going ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         polarity  ...                                               text\n",
              "1474686         1  ...  @scarlettstvitus It helps me to put a bag of i...\n",
              "867725          1  ...  @tommcfly it's actually law of humans' life......\n",
              "191053          0  ...  @ryanseacrest Son of a crap!  I thought Adam L...\n",
              "266955          0  ...  Alright folks I think I'm going to go try and ...\n",
              "1472945         1  ...  loves that there's so much Federer love going ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHkz2XrI5U_f"
      },
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "for i in a_train.text:\n",
        "    x_train.append(i)\n",
        "for i in a_test.text:\n",
        "    x_test.append(i)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "y_train = a_train.polarity.reset_index()['polarity']\n",
        "y_test = a_test.polarity.reset_index()['polarity']\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBHrdaNWnJVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d9874e-3973-4ade-c827-6d8f36214d70"
      },
      "source": [
        "y_test.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    160002\n",
              "0    159998\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E05AweFu0Imt"
      },
      "source": [
        "# Perform reverse word lookup and make it callable\n",
        "reverse_word_index = dict([(key,tokenizer.index_word[key]) for key in tokenizer.index_word])\n",
        "reverse_word_index[0] = \"<PAD>\"\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index[i] for i in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNYcwMITFHbC",
        "outputId": "be4a9f82-a1e0-42b6-a086-f44b8db2be9b"
      },
      "source": [
        "# Concatonate test and training datasets\n",
        "allreviews = np.concatenate((x_train, x_test), axis=0)\n",
        "# Review lengths across test and training whole datasets\n",
        "print(\"Maximum review length: {}\".format(len(max((allreviews), key=len))))\n",
        "print(\"Minimum review length: {}\".format(len(min((allreviews), key=len))))\n",
        "result = [len(x) for x in allreviews]\n",
        "print(\"Mean review length: {}\".format(np.mean(result)))\n",
        "\n",
        "# Print a review and it's class as stored in the dataset. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Machine readable Review\")\n",
        "print(\"  Review Text: \" + str(x_train[60]))\n",
        "print(\"  Review Sentiment: \" + str(y_train[60]))\n",
        "\n",
        "# Print a review and it's class in human readable format. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Human Readable Review\")\n",
        "print(\"  Review Text: \" + decode_review(x_train[60]))\n",
        "print(\"  Review Sentiment: \" + class_names[y_train[60]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 63\n",
            "Minimum review length: 0\n",
            "Mean review length: 13.060979375\n",
            "\n",
            "Machine readable Review\n",
            "  Review Text: [84847, 1203, 18892, 20, 345]\n",
            "  Review Sentiment: 0\n",
            "\n",
            "Human Readable Review\n",
            "  Review Text: @internacionalj whatever dawg just leave\n",
            "  Review Sentiment: Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyfQqKod7koB"
      },
      "source": [
        "# The length of reviews\n",
        "review_length = len(max((allreviews), key=len))\n",
        "\n",
        "# Padding / truncated our reviews\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = review_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen = review_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP1MHxzW-lUx",
        "outputId": "d9c143d6-ccb5-4b3a-f1d5-79c98c213c4c"
      },
      "source": [
        "print(\"Shape Training Review Data: \" + str(x_train.shape))\n",
        "print(\"Shape Training Class Data: \" + str(y_train.shape))\n",
        "print(\"Shape Test Review Data: \" + str(x_test.shape))\n",
        "print(\"Shape Test Class Data: \" + str(y_test.shape))\n",
        "print(\"Review data\"+str(x_train[60]))\n",
        "print()\n",
        "print(\"Human Readable Review Text (post padding):\\n\" + decode_review(x_train[60]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Training Review Data: (1280000, 63)\n",
            "Shape Training Class Data: (1280000,)\n",
            "Shape Test Review Data: (320000, 63)\n",
            "Shape Test Class Data: (320000,)\n",
            "Review data[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0 84847  1203\n",
            " 18892    20   345]\n",
            "\n",
            "Human Readable Review Text (post padding):\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> @internacionalj whatever dawg just leave\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLghhjKo3x1N",
        "outputId": "194d1342-b728-413c-d8ca-203f557b5973"
      },
      "source": [
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1017110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfOdV_VCCFee"
      },
      "source": [
        "## Create and build LSTM Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nmO8M4aCKwT",
        "outputId": "12c1f802-44be-45da-af93-8f5798c555d8"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = 32,input_length = review_length))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "model.add(tf.keras.layers.LSTM(units=32))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "model.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 63, 32)            32547520  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 63, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 32,555,873\n",
            "Trainable params: 32,555,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xx1Q2I8WNI9"
      },
      "source": [
        "## Visualise the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "cz0Erj2WU3Vh",
        "outputId": "9bb5b54f-006b-4de1-84b4-94cdb68f3e37"
      },
      "source": [
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv3gqJdnEB84",
        "outputId": "9630acba-93ce-4170-b3c9-df1cd2c1fd5d"
      },
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    640002\n",
              "1    639998\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KdfAoHsGwzo"
      },
      "source": [
        "## Train the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB8KhSgl5rfB",
        "outputId": "73a18c09-ee2e-4da5-98d8-0f7dd54cc17c"
      },
      "source": [
        "history = model.fit(x_train,y_train,batch_size=1000,epochs=2,validation_split=0.2,verbose=1) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1024/1024 [==============================] - 623s 606ms/step - loss: 0.5150 - accuracy: 0.7413 - val_loss: 0.4191 - val_accuracy: 0.8076\n",
            "Epoch 2/2\n",
            "1024/1024 [==============================] - 585s 571ms/step - loss: 0.3460 - accuracy: 0.8505 - val_loss: 0.4334 - val_accuracy: 0.8030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpCS2-jFH1KY"
      },
      "source": [
        "## Evaluate model with test data and view results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPnfxwbnITqV",
        "outputId": "83127c1f-92e2-4bbf-e62a-72a852f933b2"
      },
      "source": [
        "# Get Model Predictions for test data\n",
        "from sklearn.metrics import classification_report\n",
        "predicted_classes = model.predict_classes(x_test)\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.80      0.80      0.80    159998\n",
            "    Positive       0.80      0.80      0.80    160002\n",
            "\n",
            "    accuracy                           0.80    320000\n",
            "   macro avg       0.80      0.80      0.80    320000\n",
            "weighted avg       0.80      0.80      0.80    320000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkfHCIVHrJni"
      },
      "source": [
        "## View some incorrect predictions\n",
        "\n",
        "Lets have a look at some of the incorrectly classified reviews. For readability we remove the padding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwKLBwBbp7zg",
        "outputId": "b670cad5-f4e0-42dc-a4c8-ed30edf92161"
      },
      "source": [
        "predicted_classes_reshaped = np.reshape(predicted_classes, 320000)\n",
        "\n",
        "incorrect = np.nonzero(predicted_classes_reshaped!=y_test.values)[0]\n",
        "\n",
        "# We select the first 10 incorrectly classified reviews\n",
        "for j, incorrect in enumerate(incorrect[0:20]):\n",
        "    \n",
        "    predicted = class_names[predicted_classes_reshaped[incorrect]]\n",
        "    actual = class_names[y_test[incorrect]]\n",
        "    human_readable_review = decode_review(x_test[incorrect])\n",
        "    \n",
        "    print(\"Incorrectly classified Test Review [\"+ str(j+1) +\"]\") \n",
        "    print(\"Test Review #\" + str(incorrect)  + \": Predicted [\"+ predicted + \"] Actual [\"+ actual + \"]\")\n",
        "    print(\"Test Review Text: \" + human_readable_review.replace(\"<PAD> \", \"\"))\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Incorrectly classified Test Review [1]\n",
            "Test Review #1: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: aww you can be my\n",
            "\n",
            "Incorrectly classified Test Review [2]\n",
            "Test Review #15: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: could use sum lovin rite now\n",
            "\n",
            "Incorrectly classified Test Review [3]\n",
            "Test Review #28: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: @gaelicwolf it'll be okay.. he won't be like this forever\n",
            "\n",
            "Incorrectly classified Test Review [4]\n",
            "Test Review #43: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: @willclarkfan22 no donuts for you\n",
            "\n",
            "Incorrectly classified Test Review [5]\n",
            "Test Review #51: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: @claudiagmodel why on sunday would love to have checked it out\n",
            "\n",
            "Incorrectly classified Test Review [6]\n",
            "Test Review #57: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: okay, i need to go have some dinner and i need a break from sitting on my ass for so long.\n",
            "\n",
            "Incorrectly classified Test Review [7]\n",
            "Test Review #59: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: felt reallyyy bad for my bestfriend. i wont be happy if you're angry cause i love you @carebear229\n",
            "\n",
            "Incorrectly classified Test Review [8]\n",
            "Test Review #64: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: @fki88 same applies for these i needed to hear that shit 4 real\n",
            "\n",
            "Incorrectly classified Test Review [9]\n",
            "Test Review #65: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: @thatshaute idk why but im so lazy &amp; head feels a bit heavier then usual lol &amp; yess! im excited.\n",
            "\n",
            "Incorrectly classified Test Review [10]\n",
            "Test Review #68: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: @banjoist123 i don't know jim - i wasn't following them. learning all the time is me.\n",
            "\n",
            "Incorrectly classified Test Review [11]\n",
            "Test Review #69: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: waiting for josh to wake up then hopefully going to clean\n",
            "\n",
            "Incorrectly classified Test Review [12]\n",
            "Test Review #76: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: mau donk ngopi with the gak diajak sih gw\n",
            "\n",
            "Incorrectly classified Test Review [13]\n",
            "Test Review #92: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: i'm twittering from my bed; nobody ppl repeatedly say when your young never goes away; always in the back of my mind\n",
            "\n",
            "Incorrectly classified Test Review [14]\n",
            "Test Review #93: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: @duncn very much so\n",
            "\n",
            "Incorrectly classified Test Review [15]\n",
            "Test Review #99: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: thank you @garrettmaine @johnmaine @kennedymaine @patmaine @jaredmaine for tonight!!! too bad there wasn't a signing... see you @ warped!\n",
            "\n",
            "Incorrectly classified Test Review [16]\n",
            "Test Review #100: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: @jordanknight does your back still hurt i know a great nurse who has offered to be your tour nurse to dw more than once!\n",
            "\n",
            "Incorrectly classified Test Review [17]\n",
            "Test Review #104: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: i'm sorry about your friend, take care of yourself though\n",
            "\n",
            "Incorrectly classified Test Review [18]\n",
            "Test Review #108: Predicted [Positive] Actual [Negative]\n",
            "Test Review Text: @faketragedycom ooo, probably get everything done!! i keep procrastinating\n",
            "\n",
            "Incorrectly classified Test Review [19]\n",
            "Test Review #112: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: i need to find one as well we should do that this weekend ;)\n",
            "\n",
            "Incorrectly classified Test Review [20]\n",
            "Test Review #119: Predicted [Negative] Actual [Positive]\n",
            "Test Review Text: did you see star trek? now i can't much action. can't wind down!!!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEKEB0DpD_8P",
        "outputId": "982490b6-2035-4d72-f1c2-a1d0b63e88d8"
      },
      "source": [
        "# Write your own review\n",
        "# review = \"insanely negative\"\n",
        "review = \"good thing\"\n",
        "\n",
        "# Encode review (replace word with integers)\n",
        "tmp = tokenizer.texts_to_sequences(review)\n",
        "\n",
        "# Ensure review is 500 words long (by padding or truncating)\n",
        "tmp_padded = sequence.pad_sequences(tmp, maxlen=review_length) \n",
        "\n",
        "# Run your processed review against the trained model\n",
        "rawprediction = model.predict(tmp_padded)[0][0]\n",
        "prediction = int(round(rawprediction))\n",
        "\n",
        "# Test the model and print the result\n",
        "print(\"Review: \" + review)\n",
        "print(\"Raw Prediction: \" + str(rawprediction))\n",
        "print(\"Predicted Class: \" + class_names[prediction])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: good thing\n",
            "Raw Prediction: 0.6936506\n",
            "Predicted Class: Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0795CPvzp27B"
      },
      "source": [
        "import sklearn.metrics as sm\n",
        "def evaluate_model_lstm(pred,y):\n",
        "  acc = sm.accuracy_score(y,pred)\n",
        "  pr = sm.precision_score(y,pred)\n",
        "  re = sm.recall_score(y,pred)\n",
        "  f1 = sm.f1_score(pred, y, average='macro')\n",
        "  return acc,pr,re,f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdJhE8ATbMRD",
        "outputId": "c3af8f11-5eec-45d0-e4f3-4b6e99064b8a"
      },
      "source": [
        "acc_lstm,pr_lstm,re_lstm,f1_lstm=evaluate_model_lstm(predicted_classes,y_test.values)\n",
        "print(\"Accuracy : \"+str(acc_lstm))\n",
        "print(\"Precision : \"+str(pr_lstm))\n",
        "print(\"Recall : \"+str(re_lstm))\n",
        "print(\"F1 Score : \"+str(f1_lstm))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.803690625\n",
            "Precision : 0.8033764336419656\n",
            "Recall : 0.8042149473131586\n",
            "F1 Score : 0.8036905689425383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTrEPzWSUPJg"
      },
      "source": [
        "metrics = pd.DataFrame(columns=['Accuracy','Precision','Recall','F1 Score'],index=['randomforest','Naive bayes','SVM','LSTM'])\n",
        "metrics.loc['randomforest']={'Accuracy':acc_rf,'Precision':pr_rf,'Recall':re_rf,'F1 Score':f1_rf}\n",
        "metrics.loc['Naive bayes']={'Accuracy':acc_nb,'Precision':pr_nb,'Recall':re_nb,'F1 Score':f1_nb}\n",
        "metrics.loc['LSTM']={'Accuracy':acc_lstm,'Precision':pr_lstm,'Recall':re_lstm,'F1 Score':f1_lstm}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "PHnOjqG2IcMI",
        "outputId": "9019c33c-86d9-430a-99e4-41e6aa368ba2"
      },
      "source": [
        "metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>randomforest</th>\n",
              "      <td>0.73945</td>\n",
              "      <td>0.726503</td>\n",
              "      <td>0.76804</td>\n",
              "      <td>0.739237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Naive bayes</th>\n",
              "      <td>0.773222</td>\n",
              "      <td>0.800344</td>\n",
              "      <td>0.728078</td>\n",
              "      <td>0.772759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LSTM</th>\n",
              "      <td>0.803691</td>\n",
              "      <td>0.803376</td>\n",
              "      <td>0.804215</td>\n",
              "      <td>0.803691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Accuracy Precision    Recall  F1 Score\n",
              "randomforest   0.73945  0.726503   0.76804  0.739237\n",
              "Naive bayes   0.773222  0.800344  0.728078  0.772759\n",
              "SVM                NaN       NaN       NaN       NaN\n",
              "LSTM          0.803691  0.803376  0.804215  0.803691"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UCIkSqoLmRD"
      },
      "source": [
        "As, I was unable to train the svm model on the entire dataset I am not including SVM results. No matter what it disconnects the colab even after changing the colab's js to keep auto clicking the connect button. However, based on the above mentioned metrics that were calculated for LSTMs, Random Forest and Naive Bayes using the entire datasetes, I see that LSTMs have outperformed the other probabilistic models. A slight increase in the performance metrics of LSTMs could be attributed to the fact that they are dense neural layers connected to one another with a short term memory i.e. LSTMs try to classify the observations by establishing a clear discrimination between positive and negative models rather than just having a simple probabilistic approach. I also believe, SVM would have performed better than Naive Bayes and random forests as SVMs are geometrical models that work better for tasks such as classification due to the fact that they do not consider the invididual observations as independent variables. This is also the same reason in LSTMs with their short term memory, that lay some relationship between different variables as this is a very important approach in fields like NLP where every word in a sentence is not usually independent of each other rather carries a meaning with interdependence.\n",
        "\n",
        "To improve the models performance we need first clean the data of all the unecessary punctuation marks and noise like @mentions. This will help the classifier to capture only the useful information. We could also remove stopwords from the sentences, but while removing the stop words proper care must be taken to make sure the action is not causing any altering of meaning in text. Stopwords are not always general, stopwords are usually specific in regard to the topic. Something like social media tweets would have specific words like \"re-tweet\",\"tweet\",\"twitter\",\"userprofile\" etc  and some general stopwords like \"the, a, an.\n",
        "\n",
        "Using Vader mentioned in the 5th question is indeed a very good way of classifying the dataset into different polarities. That is because VADERr is a lexicon and rule-based sentiment analysis approach that is specifically trained to identify sentiments in social media text. The VADER model of nltk is pretrained on enormous social media data and as our  data focuses on social media tweets training and testing for sentiment analysis, something like VADER that is already pretrained on huge data will be a good approach as compared to the other three. Also because VADER can quantify emotion and intensity of emotion in a text along with understanding emoticons, slangs, punctuations, that makes it a good option. However, in cases were the dataset is atypical and not in the lines of what VADER was used for, then it would not serve for a good purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkligJlKLal-"
      },
      "source": [
        "# **5.**\n",
        "\n",
        "Add to the comparison of #4 a the manually calculated precision, recall and F1 score\n",
        "using VADER and their suggested defaults to categorize the test split tweets in positive\n",
        "or negative. Answer the following questions: Is this approach as good as the previous\n",
        "ones? Why do you think this is? (30 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj4YtHHBV2pR"
      },
      "source": [
        "Comparision mentioned in the previous answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwgGNXxhLiSJ",
        "outputId": "c36ffd63-1267-47a8-d167-28507cd1d9dd"
      },
      "source": [
        "nltk.download('vader_lexicon')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNA7RqCzNczJ"
      },
      "source": [
        "a_train,a_test = train_test_split(train[(train['polarity']==0) | (train['polarity']==1)], test_size=0.20, random_state=seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3A2ctlhNNcq"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "sia = SIA()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "VJv2UBMmaFoH",
        "outputId": "aead09cd-d4dd-429c-a3fc-30de32ac0258"
      },
      "source": [
        "a_test = a_test.drop(['date','query','user','id'],axis=1)\n",
        "a_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>402563</th>\n",
              "      <td>0</td>\n",
              "      <td>is going to bed now after a day of hard work -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558129</th>\n",
              "      <td>1</td>\n",
              "      <td>@lilxicanita aww mecheee you can be my friend....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415201</th>\n",
              "      <td>1</td>\n",
              "      <td>having my big brother open for me is a dream c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112565</th>\n",
              "      <td>1</td>\n",
              "      <td>acting going well. will reach goal by God's gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706645</th>\n",
              "      <td>0</td>\n",
              "      <td>@morganmovement what?! Wtf happened?! I'm doin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087432</th>\n",
              "      <td>1</td>\n",
              "      <td>@mileycyrus http://twitpic.com/5ppwd - I have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1520516</th>\n",
              "      <td>1</td>\n",
              "      <td>@gin_lady thanks, lots of sleep and knitting s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728046</th>\n",
              "      <td>0</td>\n",
              "      <td>just watched like a three hour movie arizona i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285604</th>\n",
              "      <td>0</td>\n",
              "      <td>I saw all the 'tussen de oren' episodes and f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311901</th>\n",
              "      <td>1</td>\n",
              "      <td>He say Im special.  *bllush</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>320000 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         polarity                                               text\n",
              "402563          0  is going to bed now after a day of hard work -...\n",
              "1558129         1  @lilxicanita aww mecheee you can be my friend....\n",
              "1415201         1  having my big brother open for me is a dream c...\n",
              "1112565         1  acting going well. will reach goal by God's gr...\n",
              "706645          0  @morganmovement what?! Wtf happened?! I'm doin...\n",
              "...           ...                                                ...\n",
              "1087432         1  @mileycyrus http://twitpic.com/5ppwd - I have ...\n",
              "1520516         1  @gin_lady thanks, lots of sleep and knitting s...\n",
              "728046          0  just watched like a three hour movie arizona i...\n",
              "285604          0   I saw all the 'tussen de oren' episodes and f...\n",
              "1311901         1                        He say Im special.  *bllush\n",
              "\n",
              "[320000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EBp7WwSW5bTD"
      },
      "source": [
        "a_test[['neg', 'neu', 'pos', 'compound']] = a_test['text'].apply(sia.polarity_scores).apply(pd.Series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh-_U1OZ8C-s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "d929338a-7d1f-4f21-82c3-1d65fa497c40"
      },
      "source": [
        "a_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "      <th>predicted_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>402563</th>\n",
              "      <td>0</td>\n",
              "      <td>is going to bed now after a day of hard work -...</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558129</th>\n",
              "      <td>1</td>\n",
              "      <td>@lilxicanita aww mecheee you can be my friend....</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415201</th>\n",
              "      <td>1</td>\n",
              "      <td>having my big brother open for me is a dream c...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.5905</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112565</th>\n",
              "      <td>1</td>\n",
              "      <td>acting going well. will reach goal by God's gr...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706645</th>\n",
              "      <td>0</td>\n",
              "      <td>@morganmovement what?! Wtf happened?! I'm doin...</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.6950</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087432</th>\n",
              "      <td>1</td>\n",
              "      <td>@mileycyrus http://twitpic.com/5ppwd - I have ...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1520516</th>\n",
              "      <td>1</td>\n",
              "      <td>@gin_lady thanks, lots of sleep and knitting s...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.6808</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728046</th>\n",
              "      <td>0</td>\n",
              "      <td>just watched like a three hour movie arizona i...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.839</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285604</th>\n",
              "      <td>0</td>\n",
              "      <td>I saw all the 'tussen de oren' episodes and f...</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.7316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311901</th>\n",
              "      <td>1</td>\n",
              "      <td>He say Im special.  *bllush</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>320000 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         polarity  ... predicted_labels\n",
              "402563          0  ...                1\n",
              "1558129         1  ...             None\n",
              "1415201         1  ...                1\n",
              "1112565         1  ...                1\n",
              "706645          0  ...                0\n",
              "...           ...  ...              ...\n",
              "1087432         1  ...             None\n",
              "1520516         1  ...                1\n",
              "728046          0  ...                1\n",
              "285604          0  ...                0\n",
              "1311901         1  ...                1\n",
              "\n",
              "[320000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXiN2FSdDIGV"
      },
      "source": [
        "Based on the data given in https://github.com/cjhutto/vaderSentiment#about-the-scoring , I am using compound score to evaluate the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB8WOszD-sHV"
      },
      "source": [
        "a_test[['predicted_labels']] = a_test['compound'].apply(lambda x : \"1\" if x >= 0.05 else (\"0\" if x <= -0.05 else \"1\"))\n",
        "# a_test.predicted_labels.astype(np.int16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "gFuLAduTGJ9q",
        "outputId": "96f49934-9325-478c-b8f3-594203806141"
      },
      "source": [
        "a_test= a_test.astype({'polarity':'int16','predicted_labels':'int16'})\n",
        "a_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "      <th>predicted_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>402563</th>\n",
              "      <td>0</td>\n",
              "      <td>is going to bed now after a day of hard work -...</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558129</th>\n",
              "      <td>1</td>\n",
              "      <td>@lilxicanita aww mecheee you can be my friend....</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415201</th>\n",
              "      <td>1</td>\n",
              "      <td>having my big brother open for me is a dream c...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.5905</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112565</th>\n",
              "      <td>1</td>\n",
              "      <td>acting going well. will reach goal by God's gr...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706645</th>\n",
              "      <td>0</td>\n",
              "      <td>@morganmovement what?! Wtf happened?! I'm doin...</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.6950</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087432</th>\n",
              "      <td>1</td>\n",
              "      <td>@mileycyrus http://twitpic.com/5ppwd - I have ...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1520516</th>\n",
              "      <td>1</td>\n",
              "      <td>@gin_lady thanks, lots of sleep and knitting s...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.6808</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728046</th>\n",
              "      <td>0</td>\n",
              "      <td>just watched like a three hour movie arizona i...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.839</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285604</th>\n",
              "      <td>0</td>\n",
              "      <td>I saw all the 'tussen de oren' episodes and f...</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.7316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311901</th>\n",
              "      <td>1</td>\n",
              "      <td>He say Im special.  *bllush</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>320000 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         polarity  ... predicted_labels\n",
              "402563          0  ...                1\n",
              "1558129         1  ...                1\n",
              "1415201         1  ...                1\n",
              "1112565         1  ...                1\n",
              "706645          0  ...                0\n",
              "...           ...  ...              ...\n",
              "1087432         1  ...                1\n",
              "1520516         1  ...                1\n",
              "728046          0  ...                1\n",
              "285604          0  ...                0\n",
              "1311901         1  ...                1\n",
              "\n",
              "[320000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUlIGwbJSD3m"
      },
      "source": [
        "t = a_test[(a_test[\"polarity\"] == a_test[\"predicted_labels\"])]\n",
        "f = a_test[(a_test[\"polarity\"] != a_test[\"predicted_labels\"])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwtumP1tSP5j",
        "outputId": "c546bcc3-0a30-40e9-ff11-75b466141100"
      },
      "source": [
        "t.count()['polarity'] + f.count()['polarity']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXtAAwMLJRsb"
      },
      "source": [
        "tp = t[t['predicted_labels']==1].count()['polarity']\n",
        "tn = t[t['predicted_labels']==0].count()['polarity']\n",
        "fp = f[f['predicted_labels']==1].count()['polarity']\n",
        "fn = f[f['predicted_labels']==0].count()['polarity']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xO3DVCDMc4n",
        "outputId": "23bbb62d-eab2-4d4a-a42a-f0ba67908ae4"
      },
      "source": [
        "tn+tp+fp+fn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fm_ai8YLNzl",
        "outputId": "4232231f-d471-4e43-8fa9-7fed78b167bd"
      },
      "source": [
        "print(\"Accuracy : \"+str((tp+tn)/(tp+tn+fp+fn)))\n",
        "print(\"Precision : \"+str(tp/(tp+fp)))\n",
        "print(\"Recall : \"+str(tp/(tp+fn)))\n",
        "print(\"F1 Score : \"+str(2*((tp/(tp+fp)*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.659321875\n",
            "Precision : 0.6075383349855519\n",
            "Recall : 0.9001137485781427\n",
            "F1 Score : 0.7254374057125299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e5mMjLNcP2H"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YREIFliZcIcQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgzEGVWyGwV9"
      },
      "source": [
        "# **Bonus (30 points):**\n",
        "\n",
        "Try the following things to improve the LSTM model:\n",
        "\n",
        "1) Use 90% training data, 10% testing\n",
        "2) Remove stopwords from the tweets.\n",
        "3) Remove all user mentions for the tweets (@something)\n",
        "Compare all three new models in terms of their precision, recall and F1 score. Answer the\n",
        "following questions: Did this change the results in any way? Why do you think so?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUR34uKQ6k7t"
      },
      "source": [
        "import regex as re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNEmlxPD2X86"
      },
      "source": [
        "# **A**\n",
        "\n",
        "90% Training data and 10% test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLKDy7K_HB4b",
        "outputId": "dc586436-1f1a-40df-c019-956705d5123c"
      },
      "source": [
        "a_train,a_test = train_test_split(train[(train['polarity']==0) | (train['polarity']==1)], test_size=0.10, random_state=seed_value)\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "s_words = stopwords.words('english')\n",
        "print(s_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FJ3czDP12F4"
      },
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "for i in a_train.text:\n",
        "    x_train.append(i)\n",
        "for i in a_test.text:\n",
        "    x_test.append(i)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "y_train = a_train.polarity.reset_index()['polarity']\n",
        "y_test = a_test.polarity.reset_index()['polarity']\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKIQTwhtIyEm"
      },
      "source": [
        "# Concatonate test and training datasets\n",
        "allreviews = np.concatenate((x_train, x_test), axis=0)\n",
        "# Review lengths across test and training whole datasets\n",
        "print(\"Maximum review length: {}\".format(len(max((allreviews), key=len))))\n",
        "print(\"Minimum review length: {}\".format(len(min((allreviews), key=len))))\n",
        "result = [len(x) for x in allreviews]\n",
        "print(\"Mean review length: {}\".format(np.mean(result)))\n",
        "\n",
        "# Print a review and it's class as stored in the dataset. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Machine readable Review\")\n",
        "print(\"  Review Text: \" + str(x_train[60]))\n",
        "print(\"  Review Sentiment: \" + str(y_train[60]))\n",
        "\n",
        "# Print a review and it's class in human readable format. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Human Readable Review\")\n",
        "print(\"  Review Text: \" + decode_review(x_train[60]))\n",
        "print(\"  Review Sentiment: \" + class_names[y_train[60]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD-cdfIj2ShE"
      },
      "source": [
        "# The length of reviews\n",
        "review_length = len(max((allreviews), key=len))\n",
        "\n",
        "# Padding / truncated our reviews\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = review_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen = review_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTwpBp_w2f20",
        "outputId": "2dc620e3-18cb-4924-c71e-82c7a768eb7e"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = 32,input_length = review_length))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "model.add(tf.keras.layers.LSTM(units=32))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "model.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 63, 32)            35413888  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 63, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 35,422,241\n",
            "Trainable params: 35,422,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgGxD1IO2nJd",
        "outputId": "e8ae6d0a-ac62-4840-e6e6-1b608fc4c5d6"
      },
      "source": [
        "history = model.fit(x_train,y_train,batch_size=1000,epochs=2,validation_split=0.2,verbose=1) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1152/1152 [==============================] - 477s 412ms/step - loss: 0.5126 - accuracy: 0.7420 - val_loss: 0.4143 - val_accuracy: 0.8103\n",
            "Epoch 2/2\n",
            "1152/1152 [==============================] - 461s 400ms/step - loss: 0.3415 - accuracy: 0.8522 - val_loss: 0.4260 - val_accuracy: 0.8064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qlPYQTm2nJe",
        "outputId": "f14698da-c110-4874-b6df-00e4be0ee2f7"
      },
      "source": [
        "# Get Model Predictions for test data\n",
        "from sklearn.metrics import classification_report\n",
        "predicted_classes = model.predict_classes(x_test)\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.80      0.82      0.81     80144\n",
            "    Positive       0.82      0.80      0.81     79856\n",
            "\n",
            "    accuracy                           0.81    160000\n",
            "   macro avg       0.81      0.81      0.81    160000\n",
            "weighted avg       0.81      0.81      0.81    160000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec3D4PWr2TtZ"
      },
      "source": [
        "# **B**\n",
        "\n",
        "Remove all user mentions for the tweets (@something)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzUfe9Ur6rvi"
      },
      "source": [
        "a_train,a_test = train_test_split(train[(train['polarity']==0) | (train['polarity']==1)], test_size=0.20, random_state=seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jTmNLIMGtNw"
      },
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "count = []\n",
        "for i in a_train.text:\n",
        "    # print(i)\n",
        "    i = re.sub(r'@\\w+ ?','', i)\n",
        "    # print(st)\n",
        "    x_train.append(i)\n",
        "for i in a_test.text:\n",
        "    # print(i)\n",
        "    i = re.sub(r'@\\w+ ?','', i)\n",
        "    # print(st)\n",
        "    x_test.append(i)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters='',\n",
        "    num_words=None,\n",
        "    lower=True,\n",
        "    split=\" \",\n",
        "    char_level=False,\n",
        ")\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "y_train = a_train.polarity.reset_index()['polarity']\n",
        "y_test = a_test.polarity.reset_index()['polarity']\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZyQHvjWI4_3"
      },
      "source": [
        "# Concatonate test and training datasets\n",
        "allreviews = np.concatenate((x_train, x_test), axis=0)\n",
        "# Review lengths across test and training whole datasets\n",
        "print(\"Maximum review length: {}\".format(len(max((allreviews), key=len))))\n",
        "print(\"Minimum review length: {}\".format(len(min((allreviews), key=len))))\n",
        "result = [len(x) for x in allreviews]\n",
        "print(\"Mean review length: {}\".format(np.mean(result)))\n",
        "\n",
        "# Print a review and it's class as stored in the dataset. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Machine readable Review\")\n",
        "print(\"  Review Text: \" + str(x_train[60]))\n",
        "print(\"  Review Sentiment: \" + str(y_train[60]))\n",
        "\n",
        "# Print a review and it's class in human readable format. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Human Readable Review\")\n",
        "print(\"  Review Text: \" + decode_review(x_train[60]))\n",
        "print(\"  Review Sentiment: \" + class_names[y_train[60]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWUAGEKmbpfP"
      },
      "source": [
        "# The length of reviews\n",
        "review_length = len(max((allreviews), key=len))\n",
        "\n",
        "# Padding / truncated our reviews\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = review_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen = review_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZscjZROheQH_",
        "outputId": "56c3a9aa-1a2a-4776-d56d-9fdcc63cbf42"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = 32,input_length = review_length))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "model.add(tf.keras.layers.LSTM(units=32))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "model.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 63, 32)            22858496  \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 63, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 22,866,849\n",
            "Trainable params: 22,866,849\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LNvKqRQeXJ2",
        "outputId": "973c11a2-15e4-4f06-c189-8369f5653b24"
      },
      "source": [
        "history = model.fit(x_train,y_train,batch_size=1000,epochs=2,validation_split=0.2,verbose=1) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1024/1024 [==============================] - 371s 360ms/step - loss: 0.5190 - accuracy: 0.7370 - val_loss: 0.4195 - val_accuracy: 0.8061\n",
            "Epoch 2/2\n",
            "1024/1024 [==============================] - 353s 344ms/step - loss: 0.3707 - accuracy: 0.8362 - val_loss: 0.4196 - val_accuracy: 0.8084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVgtV6UJ5NfL",
        "outputId": "13048da2-bcee-40a3-9dea-f67ccabaee35"
      },
      "source": [
        "# Get Model Predictions for test data\n",
        "from sklearn.metrics import classification_report\n",
        "predicted_classes = model.predict_classes(x_test)\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.81      0.81      0.81    159998\n",
            "    Positive       0.81      0.81      0.81    160002\n",
            "\n",
            "    accuracy                           0.81    320000\n",
            "   macro avg       0.81      0.81      0.81    320000\n",
            "weighted avg       0.81      0.81      0.81    320000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KhYccw15GNA"
      },
      "source": [
        "# **C**\n",
        "\n",
        "Remove stopwords from the tweets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0thDoynr9mYD",
        "outputId": "70b8dc20-6764-4727-f14e-1699beeb020c"
      },
      "source": [
        "a_train,a_test = train_test_split(train[(train['polarity']==0) | (train['polarity']==1)], test_size=0.20, random_state=seed_value)\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "s_words = stopwords.words('english')\n",
        "print(s_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqNjg7cy9mYE"
      },
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "count = []\n",
        "for i in a_train.text:\n",
        "    # print(i)\n",
        "    st=\"\"\n",
        "    st =\" \".join([word for word in nltk.word_tokenize(i) if word not in s_words])\n",
        "    # print(st)\n",
        "    x_train.append(st)\n",
        "for i in a_test.text:\n",
        "    # print(i)\n",
        "    st=\"\"\n",
        "    st =\" \".join([word for word in nltk.word_tokenize(i) if word not in s_words])\n",
        "    # print(st)\n",
        "    x_test.append(st)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "y_train = a_train.polarity.reset_index()['polarity']\n",
        "y_test = a_test.polarity.reset_index()['polarity']\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvvAdVfS9mYE"
      },
      "source": [
        "# The length of reviews\n",
        "review_length = 100\n",
        "\n",
        "# Padding / truncated our reviews\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = review_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen = review_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhaKz9q79mYF",
        "outputId": "8cadfd6e-796e-48dd-e727-79ff3bde1afe"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = 32,input_length = review_length))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "model.add(tf.keras.layers.LSTM(units=32))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "model.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 32)           22132576  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 22,140,929\n",
            "Trainable params: 22,140,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QarDx2UB9mYG",
        "outputId": "28252947-6a27-4f81-df6a-9918418c4dbc"
      },
      "source": [
        "history = model.fit(x_train,y_train,batch_size=1000,epochs=2,validation_split=0.2,verbose=1) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1024/1024 [==============================] - 466s 453ms/step - loss: 0.5251 - accuracy: 0.7366 - val_loss: 0.4349 - val_accuracy: 0.7981\n",
            "Epoch 2/2\n",
            "1024/1024 [==============================] - 459s 448ms/step - loss: 0.3902 - accuracy: 0.8265 - val_loss: 0.4414 - val_accuracy: 0.7964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXq2H1vc9mYH",
        "outputId": "f0684ac8-a8b5-4492-e8b1-b967d112671f"
      },
      "source": [
        "# Get Model Predictions for test data\n",
        "from sklearn.metrics import classification_report\n",
        "predicted_classes = model.predict_classes(x_test)\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.80      0.80    159998\n",
            "    Positive       0.80      0.79      0.80    160002\n",
            "\n",
            "    accuracy                           0.80    320000\n",
            "   macro avg       0.80      0.80      0.80    320000\n",
            "weighted avg       0.80      0.80      0.80    320000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VyoPGUqXEun"
      },
      "source": [
        "There is a definite increase of performace after using 90% of data for training as compared to the 80% initially. The same is observed with the removal of @mentions. This is because neural networks or any deep learning models always perform better with more data. The more is the training data the better are the estimates or outputs (in our case classes). Removal of @mentions sort of cleans the data and thus results in a slight increase of performance. However, the removal of stopwords has not shown any significant improvement rather a slight decrease in performance. I believe it is so because social media text is not usually plain english, different people speak in different styles. And saying something like \"I did not like his behaviour\" vs \"I hate his behaviour\" though convey the same meaning but once are devoid of stopwords would become \"like behaviour\" and \"hate behaviour\". This completely alters the meaning of the text. So, during the course of training such text classifier would have misunderstood few sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noT1VlgFaOPQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}